{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVeImQncKoECT0TarN2csV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccspen21/greenland-fishery-nowcast-2025/blob/main/periodic_update.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "Kg2LslY3Up8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7ede3bf-b846-4290-bf3c-1b6cfb75f009"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pyjstat in /usr/local/lib/python3.11/dist-packages (2.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# CELL 1: SETUP AND IMPORTS\n",
        "!pip install requests pandas pyjstat\n",
        "import os\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import requests\n",
        "from pyjstat import pyjstat\n",
        "from urllib.parse import quote\n",
        "from io import StringIO\n",
        "from IPython.display import display\n",
        "from google.colab import drive\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define database and script paths\n",
        "DB_PATH = '/content/drive/MyDrive/greenland_fishery.db'\n",
        "DDL_PATH = '/content/drive/MyDrive/ddl.sql'\n",
        "DML_PATH = '/content/drive/MyDrive/dml_populate.sql'\n",
        "\n",
        "# Helper function to validate DataFrame\n",
        "def validate_dataframe(df, expected_columns, dtypes):\n",
        "    if df.empty:\n",
        "        raise ValueError(\"DataFrame is empty, no rows found.\")\n",
        "    if not all(col in df.columns for col in expected_columns):\n",
        "        raise ValueError(f\"DataFrame missing expected columns: {expected_columns}\")\n",
        "    for col, dtype in dtypes.items():\n",
        "        if col in df.columns:\n",
        "            if dtype == int:\n",
        "                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)\n",
        "            else:\n",
        "                df[col] = df[col].astype(dtype)\n",
        "    if df.isnull().any().any():\n",
        "        raise ValueError(f\"DataFrame contains NaN values: {df.head()}\")\n",
        "\n",
        "# Helper function for API calls with retries\n",
        "def fetch_with_retries(url, max_retries=3, timeout=60, method='get', json=None):\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            if method == 'get':\n",
        "                response = requests.get(url, timeout=timeout)\n",
        "            else:\n",
        "                response = requests.post(url, json=json, timeout=timeout)\n",
        "            response.raise_for_status()\n",
        "            return response\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Attempt {attempt+1} failed: {e}\")\n",
        "            if attempt + 1 == max_retries:\n",
        "                raise\n",
        "            time.sleep(2 ** attempt)\n",
        "\n",
        "# Helper function to get latest available quarter from bank.stat.gl\n",
        "def get_latest_quarter_from_api(url, nation=None, unit=None, branch=None):\n",
        "    query = {\n",
        "        \"query\": [\n",
        "            {\"code\": \"time\", \"selection\": {\"filter\": \"all\", \"values\": [\"*\"]}},\n",
        "            {\"code\": \"quarter\", \"selection\": {\"filter\": \"all\", \"values\": [\"*\"]}}\n",
        "        ],\n",
        "        \"response\": {\"format\": \"json-stat2\"}\n",
        "    }\n",
        "    if nation:\n",
        "        query[\"query\"].insert(0, {\"code\": \"nation\", \"selection\": {\"filter\": \"item\", \"values\": [nation]}})\n",
        "    if unit:\n",
        "        query[\"query\"].append({\"code\": \"unit\", \"selection\": {\"filter\": \"item\", \"values\": [unit]}})\n",
        "    if branch:\n",
        "        query[\"query\"].insert(0, {\"code\": \"branch\", \"selection\": {\"filter\": \"item\", \"values\": [branch]}})\n",
        "    try:\n",
        "        response = fetch_with_retries(url, max_retries=3, timeout=60, method='post', json=query)\n",
        "        dataset = pyjstat.Dataset.read(response.text)\n",
        "        df = dataset.write('dataframe')\n",
        "        df[\"time\"] = df[\"time\"].astype(int)\n",
        "        latest_year = df[\"time\"].max()\n",
        "        latest_quarter = df[df[\"time\"] == latest_year][\"quarter\"].max()\n",
        "        # Clean quarter format to match \"Q1\", \"Q2\", etc.\n",
        "        latest_quarter = latest_quarter.replace(\"Quarter \", \"Q\").replace(\"quarter \", \"Q\")\n",
        "        return latest_year, latest_quarter\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching latest quarter: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Helper function to get the latest available time point from ERDDAP\n",
        "def get_latest_erddap_time(dataset_id, lat_deg, lon_deg):\n",
        "    # Query a single point for the most recent time to minimize data transfer\n",
        "    base = f\"https://coastwatch.pfeg.noaa.gov/erddap/griddap/{dataset_id}.csv?\"\n",
        "    var = \"sst\"\n",
        "    time = \"[(1981-01-01T00:00:00Z):1:(last)]\"  # From 1981 to the last available time\n",
        "    zlev = \"[0:1:0]\"\n",
        "    lat = f\"[({lat_deg}):1:({lat_deg})]\"  # Single latitude point in degrees\n",
        "    lon = f\"[({lon_deg}):1:({lon_deg})]\"  # Single longitude point in degrees\n",
        "    query = f\"{var}{time}{zlev}{lat}{lon}\"\n",
        "    full_url = base + quote(query, safe=\":/[](),-T\")\n",
        "    try:\n",
        "        response = fetch_with_retries(full_url, max_retries=3, timeout=60, method='get')\n",
        "        df = pd.read_csv(StringIO(response.text), skiprows=[1])\n",
        "        df = df.rename(columns={col: col.strip() for col in df.columns})\n",
        "        if df.empty or \"time\" not in df.columns:\n",
        "            raise ValueError(\"No valid time data returned from ERDDAP.\")\n",
        "        latest_time = pd.to_datetime(df[\"time\"]).max()\n",
        "        year = latest_time.year\n",
        "        quarter = \"Q\" + str((latest_time.month - 1) // 3 + 1)\n",
        "        return year, quarter\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching latest ERDDAP time: {e}\")\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to SQLite database\n",
        "conn = sqlite3.connect(DB_PATH)\n",
        "cursor = conn.cursor()\n",
        "print(f\"Connected to SQLite database at {DB_PATH}\")\n",
        "\n",
        "# Function to execute SQL scripts\n",
        "def execute_sql_script(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            sql_script = file.read()\n",
        "        cursor.executescript(sql_script)\n",
        "        conn.commit()\n",
        "        print(f\"Successfully executed SQL script: {file_path}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"SQL script {file_path} not found, creating if DML.\")\n",
        "        if file_path == DML_PATH:\n",
        "            with open(file_path, 'w') as f:\n",
        "                f.write(\"\")\n",
        "            print(f\"Created empty DML script at {file_path}\")\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"Required SQL script {file_path} not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error executing SQL script {file_path}: {e}\")\n",
        "        raise\n",
        "\n",
        "# Ensure database schema exists\n",
        "def ensure_schema():\n",
        "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
        "    tables = cursor.fetchall()\n",
        "    if not tables:\n",
        "        print(\"No tables found in database, executing ddl.sql to create schema.\")\n",
        "        if not os.path.exists(DDL_PATH):\n",
        "            raise FileNotFoundError(f\"DDL script {DDL_PATH} not found. Please ensure setup_dataset.ipynb has been run.\")\n",
        "        execute_sql_script(DDL_PATH)\n",
        "    else:\n",
        "        print(\"Tables found in database:\", [table[0] for table in tables])\n",
        "\n",
        "# Validate table schema\n",
        "def validate_table_schema(table_name, expected_columns):\n",
        "    cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
        "    columns = [col[1] for col in cursor.fetchall()]\n",
        "    if not all(col in columns for col in expected_columns):\n",
        "        raise ValueError(f\"Table {table_name} schema mismatch. Expected columns {expected_columns}, found {columns}\")\n",
        "\n",
        "# Function to check for existing data\n",
        "def check_existing_data(table_name, year, quarter, expected_columns=None):\n",
        "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name=?\", (table_name,))\n",
        "    if not cursor.fetchone():\n",
        "        print(f\"Table {table_name} does not exist.\")\n",
        "        return False\n",
        "    if expected_columns:\n",
        "        validate_table_schema(table_name, expected_columns)\n",
        "    query = f\"SELECT COUNT(*) FROM {table_name} WHERE Year = ? AND Quarter = ?\"\n",
        "    cursor.execute(query, (year, quarter))\n",
        "    count = cursor.fetchone()[0]\n",
        "    return count > 0\n",
        "\n",
        "# Function to get latest data point from table\n",
        "def get_latest_db_data(table_name):\n",
        "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name=?\", (table_name,))\n",
        "    if not cursor.fetchone():\n",
        "        print(f\"Table {table_name} does not exist, assuming no data.\")\n",
        "        return 2010, \"Q4\"\n",
        "    # Fetch the latest year and corresponding quarter\n",
        "    query = \"\"\"\n",
        "    SELECT Year, Quarter\n",
        "    FROM {table_name}\n",
        "    ORDER BY Year DESC,\n",
        "             CASE Quarter\n",
        "                 WHEN 'Q1' THEN 1\n",
        "                 WHEN 'Q2' THEN 2\n",
        "                 WHEN 'Q3' THEN 3\n",
        "                 WHEN 'Q4' THEN 4\n",
        "             END DESC\n",
        "    LIMIT 1\n",
        "    \"\"\"\n",
        "    cursor.execute(query.format(table_name=table_name))\n",
        "    result = cursor.fetchone()\n",
        "    return result if result and result[0] else (2010, \"Q4\")\n",
        "\n",
        "# Initialize database\n",
        "ensure_schema()"
      ],
      "metadata": {
        "id": "XsZIYHGVVYxT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc005a9a-e4c3-41b7-f4e8-042a734205c6"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to SQLite database at /content/drive/MyDrive/greenland_fishery.db\n",
            "Tables found in database: ['ice_melt_sst', 'total_catch', 'fish_exports', 'sst_west', 'sst_east', 'sst_south', 'foreign_catch']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define quarter order\n",
        "quarter_order = [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]\n",
        "\n",
        "# Get latest data point from database\n",
        "latest_year, latest_quarter = get_latest_db_data('total_catch')\n",
        "print(f\"Latest database data point: {latest_year} {latest_quarter}\")\n",
        "\n",
        "# Check API for latest available data\n",
        "url = \"https://bank.stat.gl:443/api/v1/en/Greenland/FI/FI10/FIX008.px\"\n",
        "api_year, api_quarter = get_latest_quarter_from_api(url, nation=\"GRL\")\n",
        "print(f\"Latest API data point: {api_year} {api_quarter}\")\n",
        "\n",
        "# Determine if update is needed\n",
        "update_needed = False\n",
        "if api_year and api_quarter:\n",
        "    db_idx = latest_year * 4 + quarter_order.index(latest_quarter)\n",
        "    api_idx = api_year * 4 + quarter_order.index(api_quarter)\n",
        "    if api_idx > db_idx:\n",
        "        update_needed = True\n",
        "        next_year, next_quarter = api_year, api_quarter\n",
        "    else:\n",
        "        print(\"No new data available for total_catch.\")\n",
        "else:\n",
        "    print(\"Could not fetch latest quarter from API, skipping total_catch update.\")\n",
        "\n",
        "if update_needed:\n",
        "    expected_columns = [\"Year\", \"Quarter\", \"Unit\", \"Total_Catch\"]\n",
        "    if not check_existing_data('total_catch', next_year, next_quarter, expected_columns):\n",
        "        # Fetch new data\n",
        "        query = {\n",
        "            \"query\": [\n",
        "                {\"code\": \"nation\", \"selection\": {\"filter\": \"item\", \"values\": [\"GRL\"]}},\n",
        "                {\"code\": \"unit\", \"selection\": {\"filter\": \"item\", \"values\": [\"Ton\"]}},\n",
        "                {\"code\": \"time\", \"selection\": {\"filter\": \"item\", \"values\": [str(next_year)]}},\n",
        "                {\"code\": \"quarter\", \"selection\": {\"filter\": \"item\", \"values\": [str(quarter_order.index(next_quarter) + 1)]}}\n",
        "            ],\n",
        "            \"response\": {\"format\": \"json-stat2\"}\n",
        "        }\n",
        "        try:\n",
        "            response = fetch_with_retries(url, max_retries=3, timeout=60, method='post', json=query)\n",
        "            dataset = pyjstat.Dataset.read(response.text)\n",
        "            df = dataset.write('dataframe')\n",
        "            print(\"Data successfully retrieved and converted to DataFrame!\")\n",
        "\n",
        "            # Clean DataFrame\n",
        "            df_new = df.copy()\n",
        "            df_new.drop(columns=['nation'], inplace=True)\n",
        "            df_new.rename(columns={\n",
        "                \"time\": \"Year\",\n",
        "                \"quarter\": \"Quarter\",\n",
        "                \"unit\": \"Unit\",\n",
        "                \"value\": \"Total_Catch\"\n",
        "            }, inplace=True)\n",
        "            df_new[\"Quarter\"] = df_new[\"Quarter\"].str.replace(\"Quarter \", \"Q\")\n",
        "            df_new[\"Quarter\"] = pd.Categorical(df_new[\"Quarter\"], categories=quarter_order, ordered=True)\n",
        "            df_new = df_new[[\"Year\", \"Quarter\", \"Unit\", \"Total_Catch\"]]\n",
        "            df_new[\"Year\"] = df_new[\"Year\"].astype(int)\n",
        "\n",
        "            # Validate\n",
        "            dtypes = {\"Year\": int, \"Quarter\": str, \"Unit\": str, \"Total_Catch\": int}\n",
        "            validate_dataframe(df_new, expected_columns, dtypes)\n",
        "\n",
        "            # Generate DML statements\n",
        "            dml_statements = []\n",
        "            for _, row in df_new.iterrows():\n",
        "                dml_statements.append(\n",
        "                    f\"INSERT INTO total_catch (Year, Quarter, Unit, Total_Catch) VALUES \"\n",
        "                    f\"({row['Year']}, '{row['Quarter']}', '{row['Unit']}', {row['Total_Catch']})\"\n",
        "                )\n",
        "\n",
        "            # Append DML statements\n",
        "            with open(DML_PATH, 'a') as f:\n",
        "                f.write(\"\\n-- Update for total_catch\\n\")\n",
        "                f.write(\"\\n\".join(dml_statements) + \";\\n\")\n",
        "\n",
        "            # Execute DML\n",
        "            execute_sql_script(DML_PATH)\n",
        "            print(\"Updated total_catch table with new data\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching new Total Catch data: {e}\")\n"
      ],
      "metadata": {
        "id": "wbDAgN3LVift",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aa999e7-c818-44d1-af58-c61a26dd28c9"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latest database data point: 2024 Q4\n",
            "Latest API data point: 2024 Q4\n",
            "No new data available for total_catch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4: UPDATE FOREIGN CATCH\n",
        "# Get latest data point from database\n",
        "latest_year, latest_quarter = get_latest_db_data('foreign_catch')\n",
        "print(f\"Latest database data point: {latest_year} {latest_quarter}\")\n",
        "\n",
        "# Check API for latest available data\n",
        "api_year, api_quarter = get_latest_quarter_from_api(url, nation=\"UDL\")\n",
        "print(f\"Latest API data point: {api_year} {api_quarter}\")\n",
        "\n",
        "# Determine if update is needed\n",
        "update_needed = False\n",
        "if api_year and api_quarter:\n",
        "    db_idx = latest_year * 4 + quarter_order.index(latest_quarter)\n",
        "    api_idx = api_year * 4 + quarter_order.index(api_quarter)\n",
        "    if api_idx > db_idx:\n",
        "        update_needed = True\n",
        "        next_year, next_quarter = api_year, api_quarter\n",
        "    else:\n",
        "        print(\"No new data available for foreign_catch.\")\n",
        "else:\n",
        "    print(\"Could not fetch latest quarter from API, skipping foreign_catch update.\")\n",
        "\n",
        "if update_needed:\n",
        "    expected_columns = [\"Year\", \"Quarter\", \"Unit\", \"Foreign_Catch\"]\n",
        "    if not check_existing_data('foreign_catch', next_year, next_quarter, expected_columns):\n",
        "        # Fetch new data\n",
        "        query = {\n",
        "            \"query\": [\n",
        "                {\"code\": \"nation\", \"selection\": {\"filter\": \"item\", \"values\": [\"UDL\"]}},\n",
        "                {\"code\": \"unit\", \"selection\": {\"filter\": \"item\", \"values\": [\"Ton\"]}},\n",
        "                {\"code\": \"time\", \"selection\": {\"filter\": \"item\", \"values\": [str(next_year)]}},\n",
        "                {\"code\": \"quarter\", \"selection\": {\"filter\": \"item\", \"values\": [str(quarter_order.index(next_quarter) + 1)]}}\n",
        "            ],\n",
        "            \"response\": {\"format\": \"json-stat2\"}\n",
        "        }\n",
        "        try:\n",
        "            response = fetch_with_retries(url, max_retries=3, timeout=60, method='post', json=query)\n",
        "            dataset = pyjstat.Dataset.read(response.text)\n",
        "            df = dataset.write('dataframe')\n",
        "            print(\"Data successfully retrieved and converted to DataFrame!\")\n",
        "\n",
        "            # Clean DataFrame\n",
        "            df_new = df.copy()\n",
        "            df_new.drop(columns=['nation'], inplace=True)\n",
        "            df_new.rename(columns={\n",
        "                \"time\": \"Year\",\n",
        "                \"quarter\": \"Quarter\",\n",
        "                \"unit\": \"Unit\",\n",
        "                \"value\": \"Foreign_Catch\"\n",
        "            }, inplace=True)\n",
        "            df_new[\"Quarter\"] = df_new[\"Quarter\"].str.replace(\"Quarter \", \"Q\")\n",
        "            df_new[\"Quarter\"] = pd.Categorical(df_new[\"Quarter\"], categories=quarter_order, ordered=True)\n",
        "            df_new = df_new[[\"Year\", \"Quarter\", \"Unit\", \"Foreign_Catch\"]]\n",
        "            df_new[\"Year\"] = df_new[\"Year\"].astype(int)\n",
        "\n",
        "            # Validate\n",
        "            dtypes = {\"Year\": int, \"Quarter\": str, \"Unit\": str, \"Foreign_Catch\": int}\n",
        "            validate_dataframe(df_new, expected_columns, dtypes)\n",
        "\n",
        "            # Generate DML statements\n",
        "            dml_statements = []\n",
        "            for _, row in df_new.iterrows():\n",
        "                dml_statements.append(\n",
        "                    f\"INSERT INTO foreign_catch (Year, Quarter, Unit, Foreign_Catch) VALUES \"\n",
        "                    f\"({row['Year']}, '{row['Quarter']}', '{row['Unit']}', {row['Foreign_Catch']})\"\n",
        "                )\n",
        "\n",
        "            # Append DML statements\n",
        "            with open(DML_PATH, 'a') as f:\n",
        "                f.write(\"\\n-- Update for foreign_catch\\n\")\n",
        "                f.write(\"\\n\".join(dml_statements) + \";\\n\")\n",
        "\n",
        "            # Execute DML\n",
        "            execute_sql_script(DML_PATH)\n",
        "            print(\"Updated foreign_catch table with new data\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching new Foreign Catch data: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPPooxNkEA4Z",
        "outputId": "6db619c3-8c2b-4953-ca52-c228f82d74c6"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latest database data point: 2024 Q4\n",
            "Latest API data point: 2024 Q4\n",
            "No new data available for foreign_catch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5: UPDATE FISH EXPORTS\n",
        "# Get latest data point from database\n",
        "latest_year, latest_quarter = get_latest_db_data('fish_exports')\n",
        "print(f\"Latest database data point: {latest_year} {latest_quarter}\")\n",
        "\n",
        "# Check API for latest available data\n",
        "url = \"https://bank.stat.gl:443/api/v1/en/Greenland/IE/IE10/IEX2PROD.px\"\n",
        "api_year, api_quarter = get_latest_quarter_from_api(url, branch=\"46\")\n",
        "print(f\"Latest API data point: {api_year} {api_quarter}\")\n",
        "\n",
        "# Determine if update is needed\n",
        "update_needed = False\n",
        "if api_year and api_quarter:\n",
        "    db_idx = latest_year * 4 + quarter_order.index(latest_quarter)\n",
        "    api_idx = api_year * 4 + quarter_order.index(api_quarter)\n",
        "    if api_idx > db_idx:\n",
        "        update_needed = True\n",
        "        next_year, next_quarter = api_year, api_quarter\n",
        "    else:\n",
        "        print(\"No new data available for fish_exports.\")\n",
        "else:\n",
        "    print(\"Could not fetch latest quarter from API, skipping fish_exports update.\")\n",
        "\n",
        "if update_needed:\n",
        "    expected_columns = [\"Year\", \"Quarter\", \"Fish_Export_Value_Million_Kr\"]\n",
        "    if not check_existing_data('fish_exports', next_year, next_quarter, expected_columns):\n",
        "        # Fetch new data\n",
        "        query = {\n",
        "            \"query\": [\n",
        "                {\"code\": \"branch\", \"selection\": {\"filter\": \"item\", \"values\": [\"46\"]}},\n",
        "                {\"code\": \"quarter\", \"selection\": {\"filter\": \"item\", \"values\": [\"1\", \"2\", \"3\", \"4\"]}},\n",
        "                {\"code\": \"time\", \"selection\": {\"filter\": \"item\", \"values\": [str(next_year)]}}\n",
        "            ],\n",
        "            \"response\": {\"format\": \"json-stat2\"}\n",
        "        }\n",
        "        try:\n",
        "            response = fetch_with_retries(url, max_retries=3, timeout=60, method='post', json=query)\n",
        "            dataset = pyjstat.Dataset.read(response.text)\n",
        "            df = dataset.write('dataframe')\n",
        "            print(\"Data successfully retrieved and converted to DataFrame!\")\n",
        "\n",
        "            # Clean DataFrame\n",
        "            df_new = df.copy()\n",
        "            df_new.rename(columns={\n",
        "                \"time\": \"Year\",\n",
        "                \"quarter\": \"Quarter\",\n",
        "                \"value\": \"Fish_Export_Value_Million_Kr\"\n",
        "            }, inplace=True)\n",
        "            # Fix quarters (case-insensitive replacement)\n",
        "            df_new[\"Quarter\"] = df_new[\"Quarter\"].str.replace(r\"[Qq]uarter \", \"Q\", regex=True)\n",
        "            quarter_order = [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]\n",
        "            df_new[\"Quarter\"] = pd.Categorical(df_new[\"Quarter\"], categories=quarter_order, ordered=True)\n",
        "            df_new = df_new[[\"Year\", \"Quarter\", \"Fish_Export_Value_Million_Kr\"]]\n",
        "            df_new[\"Year\"] = df_new[\"Year\"].astype(int)\n",
        "\n",
        "            # Convert export value to million Kr and round\n",
        "            df_new[\"Fish_Export_Value_Million_Kr\"] = df_new[\"Fish_Export_Value_Million_Kr\"] / 1e6\n",
        "            df_new[\"Fish_Export_Value_Million_Kr\"] = df_new[\"Fish_Export_Value_Million_Kr\"].round(0).astype(int)\n",
        "\n",
        "            # Validate\n",
        "            dtypes = {\"Year\": int, \"Quarter\": str, \"Fish_Export_Value_Million_Kr\": int}\n",
        "            validate_dataframe(df_new, expected_columns, dtypes)\n",
        "\n",
        "            # Insert directly into database\n",
        "            df_new.to_sql('fish_exports', conn, if_exists='append', index=False)\n",
        "            print(\"Inserted new data into fish_exports table\")\n",
        "\n",
        "            # Generate DML statements\n",
        "            dml_statements = []\n",
        "            for _, row in df_new.iterrows():\n",
        "                dml_statements.append(\n",
        "                    f\"INSERT INTO fish_exports (Year, Quarter, Fish_Export_Value_Million_Kr) VALUES \"\n",
        "                    f\"({row['Year']}, '{row['Quarter']}', {row['Fish_Export_Value_Million_Kr']})\"\n",
        "                )\n",
        "\n",
        "            # Append DML statements\n",
        "            with open(DML_PATH, 'a') as f:\n",
        "                f.write(\"\\n-- Update for fish_exports\\n\")\n",
        "                f.write(\"\\n\".join(dml_statements) + \";\\n\")\n",
        "\n",
        "            print(\"Updated fish_exports table with new data\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching new Fish Exports data: {e}\")\n"
      ],
      "metadata": {
        "id": "Gxopz6AnVoaw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93cd6a35-baa0-4422-a9bc-f3408ca71a5c"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latest database data point: 2024 Q4\n",
            "Latest API data point: 2024 Q4\n",
            "No new data available for fish_exports.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 6: UPDATE WEST GREENLAND SST\n",
        "# Define quarter to months mapping\n",
        "quarter_to_months = {\n",
        "    \"Q1\": (\"01-01\", \"03-31\"),\n",
        "    \"Q2\": (\"04-01\", \"06-30\"),\n",
        "    \"Q3\": (\"07-01\", \"09-30\"),\n",
        "    \"Q4\": (\"10-01\", \"12-31\")\n",
        "}\n",
        "\n",
        "# Degree to ERDDAP grid index conversion (using direct degrees as in setup_dataset.ipynb)\n",
        "def deg_to_index_lat(lat): return lat  # Direct degrees\n",
        "def deg_to_index_lon(lon): return lon  # Direct degrees\n",
        "\n",
        "# Define bounding box for West Greenland in degrees\n",
        "bbox_deg_west = {\n",
        "    'lat_min': 65.0,\n",
        "    'lat_max': 70.0,\n",
        "    'lon_min': -55.0,\n",
        "    'lon_max': -50.0\n",
        "}\n",
        "\n",
        "# Use degrees directly\n",
        "bbox_idx_west = {\n",
        "    'lat_min': deg_to_index_lat(bbox_deg_west['lat_min']),\n",
        "    'lat_max': deg_to_index_lat(bbox_deg_west['lat_max']),\n",
        "    'lon_min': deg_to_index_lon(bbox_deg_west['lon_min']),\n",
        "    'lon_max': deg_to_index_lon(bbox_deg_west['lon_max'])\n",
        "}\n",
        "\n",
        "# Get latest data point from database\n",
        "latest_db_year, latest_db_quarter = get_latest_db_data('sst_west')\n",
        "print(f\"Latest database data point: {latest_db_year} {latest_db_quarter}\")\n",
        "\n",
        "# Get latest available data point from ERDDAP\n",
        "dataset_id = \"ncdc-oisst-v2-1r\"\n",
        "api_year, api_quarter = get_latest_erddap_time(dataset_id, bbox_idx_west['lat_min'], bbox_idx_west['lon_min'])\n",
        "print(f\"Latest ERDDAP data point: {api_year} {api_quarter}\")\n",
        "\n",
        "# Determine if update is needed\n",
        "update_needed = False\n",
        "next_year, next_quarter = None, None\n",
        "if api_year and api_quarter:\n",
        "    db_idx = latest_db_year * 4 + quarter_order.index(latest_db_quarter)\n",
        "    api_idx = api_year * 4 + quarter_order.index(api_quarter)\n",
        "    if api_idx > db_idx:\n",
        "        update_needed = True\n",
        "        next_year, next_quarter = api_year, api_quarter\n",
        "        # Check if enough time has passed for the quarter\n",
        "        current_date = datetime.now()\n",
        "        quarter_end_dates = {\n",
        "            \"Q1\": datetime(next_year, 3, 31),\n",
        "            \"Q2\": datetime(next_year, 6, 30),\n",
        "            \"Q3\": datetime(next_year, 9, 30),\n",
        "            \"Q4\": datetime(next_year, 12, 31)\n",
        "        }\n",
        "        if current_date <= quarter_end_dates[next_quarter]:\n",
        "            print(f\"Current date {current_date.date()} is before {next_quarter} end ({quarter_end_dates[next_quarter].date()}), skipping sst_west update.\")\n",
        "            update_needed = False\n",
        "    else:\n",
        "        print(\"No new data available for sst_west.\")\n",
        "else:\n",
        "    print(\"Could not fetch latest data point from ERDDAP, skipping sst_west update.\")\n",
        "\n",
        "sst_west_updated = False\n",
        "if update_needed:\n",
        "    print(f\"Fetching data for: {next_year} {next_quarter}\")\n",
        "    expected_columns = [\"Year\", \"Quarter\", \"Sea_Surface_Temp_C_West\", \"Melt_Active_West\", \"Melt_Index_West\"]\n",
        "    if not check_existing_data('sst_west', next_year, next_quarter, expected_columns):\n",
        "        # Map quarter to months\n",
        "        start_month, end_month = quarter_to_months[next_quarter]\n",
        "\n",
        "        # Fetch new data\n",
        "        try:\n",
        "            base = f\"https://coastwatch.pfeg.noaa.gov/erddap/griddap/{dataset_id}.csv?\"\n",
        "            var = \"sst\"\n",
        "            time = f\"[({next_year}-{start_month}T00:00:00Z):1:({next_year}-{end_month}T00:00:00Z)]\".replace(\" \", \"\")\n",
        "            zlev = \"[0:1:0]\"\n",
        "            lat = f\"[({bbox_idx_west['lat_min']}):1:({bbox_idx_west['lat_max']})]\"\n",
        "            lon = f\"[({bbox_idx_west['lon_min']}):1:({bbox_idx_west['lon_max']})]\"\n",
        "            query = f\"{var}{time}{zlev}{lat}{lon}\"\n",
        "            full_url = base + quote(query, safe=\":/[](),-T\")\n",
        "            print(\"Constructed URL:\", full_url)\n",
        "\n",
        "            response = fetch_with_retries(full_url, max_retries=3, timeout=60, method='get')\n",
        "            df = pd.read_csv(StringIO(response.text), skiprows=[1])\n",
        "            df = df.rename(columns={col: col.strip() for col in df.columns})\n",
        "            df = df.dropna(subset=[\"sst\"])\n",
        "\n",
        "            df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
        "            df[\"Year\"] = df[\"time\"].dt.year.astype(int)\n",
        "            df[\"Quarter\"] = \"Q\" + df[\"time\"].dt.quarter.astype(str)\n",
        "\n",
        "            df_new = df.groupby(['Year', 'Quarter'])[\"sst\"].mean().reset_index()\n",
        "            df_new = df_new.rename(columns={\"sst\": \"Sea_Surface_Temp_C_West\"})\n",
        "\n",
        "            df_new[\"Melt_Active_West\"] = (df_new[\"Sea_Surface_Temp_C_West\"] > 0.5).astype(int)\n",
        "            df_new[\"Melt_Index_West\"] = df_new[\"Sea_Surface_Temp_C_West\"].clip(lower=0, upper=4) / 4\n",
        "\n",
        "            # Validate\n",
        "            dtypes = {\"Year\": int, \"Quarter\": str, \"Sea_Surface_Temp_C_West\": float, \"Melt_Active_West\": int, \"Melt_Index_West\": float}\n",
        "            validate_dataframe(df_new, expected_columns, dtypes)\n",
        "\n",
        "            # Insert directly into database\n",
        "            df_new.to_sql('sst_west', conn, if_exists='append', index=False)\n",
        "            print(\"Inserted new data into sst_west table\")\n",
        "\n",
        "            # Generate DML statements\n",
        "            dml_statements = []\n",
        "            for _, row in df_new.iterrows():\n",
        "                dml_statements.append(\n",
        "                    f\"INSERT INTO sst_west (Year, Quarter, Sea_Surface_Temp_C_West, Melt_Active_West, Melt_Index_West) VALUES \"\n",
        "                    f\"({row['Year']}, '{row['Quarter']}', {row['Sea_Surface_Temp_C_West']}, {row['Melt_Active_West']}, {row['Melt_Index_West']})\"\n",
        "                )\n",
        "\n",
        "            # Append DML statements\n",
        "            with open(DML_PATH, 'a') as f:\n",
        "                f.write(\"\\n-- Update for sst_west\\n\")\n",
        "                f.write(\"\\n\".join(dml_statements) + \";\\n\")\n",
        "\n",
        "            sst_west_updated = True\n",
        "            print(\"Updated sst_west table with new data\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching new SST West data: {e}\")\n",
        "            print(f\"Data for {next_year} {next_quarter} may not be available yet, skipping sst_west update.\")\n",
        "\n",
        "# Display updated data\n",
        "print(\"Updated SST West DataFrame:\")\n",
        "df_updated = pd.read_sql_query(\"SELECT * FROM sst_west WHERE Year = ? AND Quarter = ?\", conn, params=(next_year, next_quarter))\n",
        "display(df_updated)"
      ],
      "metadata": {
        "id": "bp6KAn4AVq0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "947f6c63-e0cf-413a-f28c-b78c78d12f3e"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latest database data point: 2024 Q4\n",
            "Attempt 1 failed: 404 Client Error:  for url: https://coastwatch.pfeg.noaa.gov/erddap/griddap/ncdc-oisst-v2-1r.csv?sst%5B(1981-01-01T00:00:00Z):1:(last)%5D%5B0:1:0%5D%5B(620):1:(620)%5D%5B(500):1:(500)%5D\n",
            "Attempt 2 failed: 404 Client Error:  for url: https://coastwatch.pfeg.noaa.gov/erddap/griddap/ncdc-oisst-v2-1r.csv?sst%5B(1981-01-01T00:00:00Z):1:(last)%5D%5B0:1:0%5D%5B(620):1:(620)%5D%5B(500):1:(500)%5D\n",
            "Attempt 3 failed: 404 Client Error:  for url: https://coastwatch.pfeg.noaa.gov/erddap/griddap/ncdc-oisst-v2-1r.csv?sst%5B(1981-01-01T00:00:00Z):1:(last)%5D%5B0:1:0%5D%5B(620):1:(620)%5D%5B(500):1:(500)%5D\n",
            "Error fetching latest ERDDAP time: 404 Client Error:  for url: https://coastwatch.pfeg.noaa.gov/erddap/griddap/ncdc-oisst-v2-1r.csv?sst%5B(1981-01-01T00:00:00Z):1:(last)%5D%5B0:1:0%5D%5B(620):1:(620)%5D%5B(500):1:(500)%5D\n",
            "Latest ERDDAP data point: None None\n",
            "Could not fetch latest data point from ERDDAP, skipping sst_west update.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 7: UPDATE EAST GREENLAND SST\n",
        "# Define bounding box for East Greenland in degrees\n",
        "bbox_deg_east = {\n",
        "    'lat_min': 65.0,\n",
        "    'lat_max': 70.0,\n",
        "    'lon_min': -40.0,\n",
        "    'lon_max': -35.0\n",
        "}\n",
        "\n",
        "# Use degrees directly (as in setup_dataset.ipynb)\n",
        "bbox_idx_east = {\n",
        "    'lat_min': deg_to_index_lat(bbox_deg_east['lat_min']),\n",
        "    'lat_max': deg_to_index_lat(bbox_deg_east['lat_max']),\n",
        "    'lon_min': deg_to_index_lon(bbox_deg_east['lon_min']),\n",
        "    'lon_max': deg_to_index_lon(bbox_deg_east['lon_max'])\n",
        "}\n",
        "\n",
        "# Get latest data point from database\n",
        "latest_db_year, latest_db_quarter = get_latest_db_data('sst_east')\n",
        "print(f\"Latest database data point: {latest_db_year} {latest_db_quarter}\")\n",
        "\n",
        "# Get latest available data point from ERDDAP\n",
        "api_year, api_quarter = get_latest_erddap_time(dataset_id, bbox_idx_east['lat_min'], bbox_idx_east['lon_min'])\n",
        "print(f\"Latest ERDDAP data point: {api_year} {api_quarter}\")\n",
        "\n",
        "# Determine if update is needed\n",
        "update_needed = False\n",
        "next_year, next_quarter = None, None\n",
        "if api_year and api_quarter:\n",
        "    db_idx = latest_db_year * 4 + quarter_order.index(latest_db_quarter)\n",
        "    api_idx = api_year * 4 + quarter_order.index(api_quarter)\n",
        "    if api_idx > db_idx:\n",
        "        update_needed = True\n",
        "        next_year, next_quarter = api_year, api_quarter\n",
        "        # Check if enough time has passed for the quarter\n",
        "        current_date = datetime.now()\n",
        "        quarter_end_dates = {\n",
        "            \"Q1\": datetime(next_year, 3, 31),\n",
        "            \"Q2\": datetime(next_year, 6, 30),\n",
        "            \"Q3\": datetime(next_year, 9, 30),\n",
        "            \"Q4\": datetime(next_year, 12, 31)\n",
        "        }\n",
        "        if current_date <= quarter_end_dates[next_quarter]:\n",
        "            print(f\"Current date {current_date.date()} is before {next_quarter} end ({quarter_end_dates[next_quarter].date()}), skipping sst_east update.\")\n",
        "            update_needed = False\n",
        "    else:\n",
        "        print(\"No new data available for sst_east.\")\n",
        "else:\n",
        "    print(\"Could not fetch latest data point from ERDDAP, skipping sst_east update.\")\n",
        "\n",
        "sst_east_updated = False\n",
        "if update_needed:\n",
        "    print(f\"Fetching data for: {next_year} {next_quarter}\")\n",
        "    expected_columns = [\"Year\", \"Quarter\", \"Sea_Surface_Temp_C_East\", \"Melt_Active_East\", \"Melt_Index_East\"]\n",
        "    if not check_existing_data('sst_east', next_year, next_quarter, expected_columns):\n",
        "        # Map quarter to months\n",
        "        start_month, end_month = quarter_to_months[next_quarter]\n",
        "\n",
        "        # Fetch new data\n",
        "        try:\n",
        "            base = f\"https://coastwatch.pfeg.noaa.gov/erddap/griddap/{dataset_id}.csv?\"\n",
        "            var = \"sst\"\n",
        "            time = f\"[({next_year}-{start_month}T00:00:00Z):1:({next_year}-{end_month}T00:00:00Z)]\".replace(\" \", \"\")\n",
        "            zlev = \"[0:1:0]\"\n",
        "            lat = f\"[({bbox_idx_east['lat_min']}):1:({bbox_idx_east['lat_max']})]\"\n",
        "            lon = f\"[({bbox_idx_east['lon_min']}):1:({bbox_idx_east['lon_max']})]\"\n",
        "            query = f\"{var}{time}{zlev}{lat}{lon}\"\n",
        "            full_url = base + quote(query, safe=\":/[](),-T\")\n",
        "            print(\"Constructed URL:\", full_url)\n",
        "\n",
        "            response = fetch_with_retries(full_url, max_retries=3, timeout=60, method='get')\n",
        "            df = pd.read_csv(StringIO(response.text), skiprows=[1])\n",
        "            df = df.rename(columns={col: col.strip() for col in df.columns})\n",
        "            df = df.dropna(subset=[\"sst\"])\n",
        "\n",
        "            df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
        "            df[\"Year\"] = df[\"time\"].dt.year.astype(int)\n",
        "            df[\"Quarter\"] = \"Q\" + df[\"time\"].dt.quarter.astype(str)\n",
        "\n",
        "            df_new = df.groupby(['Year', 'Quarter'])[\"sst\"].mean().reset_index()\n",
        "            df_new = df_new.rename(columns={\"sst\": \"Sea_Surface_Temp_C_East\"})\n",
        "\n",
        "            df_new[\"Melt_Active_East\"] = (df_new[\"Sea_Surface_Temp_C_East\"] > 0.5).astype(int)\n",
        "            df_new[\"Melt_Index_East\"] = df_new[\"Sea_Surface_Temp_C_East\"].clip(lower=0, upper=4) / 4\n",
        "\n",
        "            # Validate\n",
        "            dtypes = {\"Year\": int, \"Quarter\": str, \"Sea_Surface_Temp_C_East\": float, \"Melt_Active_East\": int, \"Melt_Index_East\": float}\n",
        "            validate_dataframe(df_new, expected_columns, dtypes)\n",
        "\n",
        "            # Insert directly into database\n",
        "            df_new.to_sql('sst_east', conn, if_exists='append', index=False)\n",
        "            print(\"Inserted new data into sst_east table\")\n",
        "\n",
        "            # Generate DML statements\n",
        "            dml_statements = []\n",
        "            for _, row in df_new.iterrows():\n",
        "                dml_statements.append(\n",
        "                    f\"INSERT INTO sst_east (Year, Quarter, Sea_Surface_Temp_C_East, Melt_Active_East, Melt_Index_East) VALUES \"\n",
        "                    f\"({row['Year']}, '{row['Quarter']}', {row['Sea_Surface_Temp_C_East']}, {row['Melt_Active_East']}, {row['Melt_Index_East']})\"\n",
        "                )\n",
        "\n",
        "            # Append DML statements\n",
        "            with open(DML_PATH, 'a') as f:\n",
        "                f.write(\"\\n-- Update for sst_east\\n\")\n",
        "                f.write(\"\\n\".join(dml_statements) + \";\\n\")\n",
        "\n",
        "            sst_east_updated = True\n",
        "            print(\"Updated sst_east table with new data\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching new SST East data: {e}\")\n",
        "            print(f\"Data for {next_year} {next_quarter} may not be available yet, skipping sst_east update.\")\n",
        "\n",
        "# Display updated data\n",
        "print(\"Updated SST East DataFrame:\")\n",
        "df_updated = pd.read_sql_query(\"SELECT * FROM sst_east WHERE Year = ? AND Quarter = ?\", conn, params=(next_year, next_quarter))\n",
        "display(df_updated)"
      ],
      "metadata": {
        "id": "Y90B8JbvxBPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 8: UPDATE SOUTH GREENLAND SST\n",
        "# Define bounding box for South Greenland in degrees\n",
        "bbox_deg_south = {\n",
        "    'lat_min': 60.0,\n",
        "    'lat_max': 65.0,\n",
        "    'lon_min': -45.0,\n",
        "    'lon_max': -40.0\n",
        "}\n",
        "\n",
        "# Use degrees directly (as in setup_dataset.ipynb)\n",
        "bbox_idx_south = {\n",
        "    'lat_min': deg_to_index_lat(bbox_deg_south['lat_min']),\n",
        "    'lat_max': deg_to_index_lat(bbox_deg_south['lat_max']),\n",
        "    'lon_min': deg_to_index_lon(bbox_deg_south['lon_min']),\n",
        "    'lon_max': deg_to_index_lon(bbox_deg_south['lon_max'])\n",
        "}\n",
        "\n",
        "# Get latest data point from database\n",
        "latest_db_year, latest_db_quarter = get_latest_db_data('sst_south')\n",
        "print(f\"Latest database data point: {latest_db_year} {latest_db_quarter}\")\n",
        "\n",
        "# Get latest available data point from ERDDAP\n",
        "api_year, api_quarter = get_latest_erddap_time(dataset_id, bbox_idx_south['lat_min'], bbox_idx_south['lon_min'])\n",
        "print(f\"Latest ERDDAP data point: {api_year} {api_quarter}\")\n",
        "\n",
        "# Determine if update is needed\n",
        "update_needed = False\n",
        "next_year, next_quarter = None, None\n",
        "if api_year and api_quarter:\n",
        "    db_idx = latest_db_year * 4 + quarter_order.index(latest_db_quarter)\n",
        "    api_idx = api_year * 4 + quarter_order.index(api_quarter)\n",
        "    if api_idx > db_idx:\n",
        "        update_needed = True\n",
        "        next_year, next_quarter = api_year, api_quarter\n",
        "        # Check if enough time has passed for the quarter\n",
        "        current_date = datetime.now()\n",
        "        quarter_end_dates = {\n",
        "            \"Q1\": datetime(next_year, 3, 31),\n",
        "            \"Q2\": datetime(next_year, 6, 30),\n",
        "            \"Q3\": datetime(next_year, 9, 30),\n",
        "            \"Q4\": datetime(next_year, 12, 31)\n",
        "        }\n",
        "        if current_date <= quarter_end_dates[next_quarter]:\n",
        "            print(f\"Current date {current_date.date()} is before {next_quarter} end ({quarter_end_dates[next_quarter].date()}), skipping sst_south update.\")\n",
        "            update_needed = False\n",
        "    else:\n",
        "        print(\"No new data available for sst_south.\")\n",
        "else:\n",
        "    print(\"Could not fetch latest data point from ERDDAP, skipping sst_south update.\")\n",
        "\n",
        "if update_needed:\n",
        "    print(f\"Fetching data for: {next_year} {next_quarter}\")\n",
        "    expected_columns = [\"Year\", \"Quarter\", \"Sea_Surface_Temp_C_South\", \"Melt_Active_South\", \"Melt_Index_South\"]\n",
        "    if not check_existing_data('sst_south', next_year, next_quarter, expected_columns):\n",
        "        # Map quarter to months\n",
        "        start_month, end_month = quarter_to_months[next_quarter]\n",
        "\n",
        "        # Fetch new data\n",
        "        try:\n",
        "            base = f\"https://coastwatch.pfeg.noaa.gov/erddap/griddap/{dataset_id}.csv?\"\n",
        "            var = \"sst\"\n",
        "            time = f\"[({next_year}-{start_month}T00:00:00Z):1:({next_year}-{end_month}T00:00:00Z)]\".replace(\" \", \"\")\n",
        "            zlev = \"[0:1:0]\"\n",
        "            lat = f\"[({bbox_idx_south['lat_min']}):1:({bbox_idx_south['lat_max']})]\"\n",
        "            lon = f\"[({bbox_idx_south['lon_min']}):1:({bbox_idx_south['lon_max']})]\"\n",
        "            query = f\"{var}{time}{zlev}{lat}{lon}\"\n",
        "            full_url = base + quote(query, safe=\":/[](),-T\")\n",
        "            print(\"Constructed URL:\", full_url)\n",
        "\n",
        "            response = fetch_with_retries(full_url, max_retries=3, timeout=60, method='get')\n",
        "            df = pd.read_csv(StringIO(response.text), skiprows=[1])\n",
        "            df = df.rename(columns={col: col.strip() for col in df.columns})\n",
        "            df = df.dropna(subset=[\"sst\"])\n",
        "\n",
        "            df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
        "            df[\"Year\"] = df[\"time\"].dt.year.astype(int)\n",
        "            df[\"Quarter\"] = \"Q\" + df[\"time\"].dt.quarter.astype(str)\n",
        "\n",
        "            df_new = df.groupby(['Year', 'Quarter'])[\"sst\"].mean().reset_index()\n",
        "            df_new = df_new.rename(columns={\"sst\": \"Sea_Surface_Temp_C_South\"})\n",
        "\n",
        "            df_new[\"Melt_Active_South\"] = (df_new[\"Sea_Surface_Temp_C_South\"] > 0.5).astype(int)\n",
        "            df_new[\"Melt_Index_South\"] = df_new[\"Sea_Surface_Temp_C_South\"].clip(lower=0, upper=4) / 4\n",
        "\n",
        "            # Validate\n",
        "            dtypes = {\"Year\": int, \"Quarter\": str, \"Sea_Surface_Temp_C_South\": float, \"Melt_Active_South\": int, \"Melt_Index_South\": float}\n",
        "            validate_dataframe(df_new, expected_columns, dtypes)\n",
        "\n",
        "            # Insert directly into database\n",
        "            df_new.to_sql('sst_south', conn, if_exists='append', index=False)\n",
        "            print(\"Inserted new data into sst_south table\")\n",
        "\n",
        "            # Generate DML statements\n",
        "            dml_statements = []\n",
        "            for _, row in df_new.iterrows():\n",
        "                dml_statements.append(\n",
        "                    f\"INSERT INTO sst_south (Year, Quarter, Sea_Surface_Temp_C_South, Melt_Active_South, Melt_Index_South) VALUES \"\n",
        "                    f\"({row['Year']}, '{row['Quarter']}', {row['Sea_Surface_Temp_C_South']}, {row['Melt_Active_South']}, {row['Melt_Index_South']})\"\n",
        "                )\n",
        "\n",
        "            # Append DML statements\n",
        "            with open(DML_PATH, 'a') as f:\n",
        "                f.write(\"\\n-- Update for sst_south\\n\")\n",
        "                f.write(\"\\n\".join(dml_statements) + \";\\n\")\n",
        "\n",
        "            print(\"Updated sst_south table with new data\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching new SST South data: {e}\")\n",
        "            print(f\"Data for {next_year} {next_quarter} may not be available yet, skipping sst_south update.\")\n",
        "\n",
        "# Display updated data\n",
        "print(\"Updated SST South DataFrame:\")\n",
        "df_updated = pd.read_sql_query(\"SELECT * FROM sst_south WHERE Year = ? AND Quarter = ?\", conn, params=(next_year, next_quarter))\n",
        "display(df_updated)"
      ],
      "metadata": {
        "id": "2cqv3IBcVrKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 9: UPDATE ICE MELT SST\n",
        "# Get latest data point from database\n",
        "latest_year, latest_quarter = get_latest_db_data('ice_melt_sst')\n",
        "print(f\"Latest database data point: {latest_year} {latest_quarter}\")\n",
        "\n",
        "# Determine next quarter\n",
        "quarter_idx = (quarter_order.index(latest_quarter) + 1) % 4\n",
        "next_year = latest_year if quarter_idx != 0 else latest_year + 1\n",
        "next_quarter = quarter_order[quarter_idx]\n",
        "\n",
        "# Check if enough time has passed\n",
        "if current_date <= quarter_end_dates[next_quarter]:\n",
        "    print(f\"Current date {current_date.date()} is before {next_quarter} end ({quarter_end_dates[next_quarter].date()}), skipping ice_melt_sst update.\")\n",
        "else:\n",
        "    print(f\"Computing data for: {next_year} {next_quarter}\")\n",
        "    # Only update if both sst_east and sst_west were updated\n",
        "    if sst_east_updated and sst_west_updated:\n",
        "        expected_columns = [\"Year\", \"Quarter\", \"Ice_Melt_Rate_East\", \"Ice_Melt_Rate_West\", \"SST_East\", \"SST_West\"]\n",
        "        if not check_existing_data('ice_melt_sst', next_year, next_quarter, expected_columns):\n",
        "            # Fetch latest SST data\n",
        "            df_sst_east = pd.read_sql_query(\"SELECT * FROM sst_east WHERE Year = ? AND Quarter = ?\", conn, params=(next_year, next_quarter))\n",
        "            df_sst_west = pd.read_sql_query(\"SELECT * FROM sst_west WHERE Year = ? AND Quarter = ?\", conn, params=(next_year, next_quarter))\n",
        "\n",
        "            if not df_sst_east.empty and not df_sst_west.empty:\n",
        "                # Normalize SST (following setup_dataset.ipynb logic)\n",
        "                sst_east_norm = (df_sst_east[\"Sea_Surface_Temp_C_East\"].iloc[0] + 2) / 22\n",
        "                sst_west_norm = (df_sst_west[\"Sea_Surface_Temp_C_West\"].iloc[0] + 2) / 22\n",
        "\n",
        "                # Compute Ice Melt Rate\n",
        "                ice_melt_rate_east = (0.7 * df_sst_east[\"Melt_Index_East\"].iloc[0] + 0.3 * sst_east_norm)\n",
        "                ice_melt_rate_west = (0.7 * df_sst_west[\"Melt_Index_West\"].iloc[0] + 0.3 * sst_west_norm)\n",
        "\n",
        "                # Create DataFrame\n",
        "                df_new = pd.DataFrame({\n",
        "                    \"Year\": [next_year],\n",
        "                    \"Quarter\": [next_quarter],\n",
        "                    \"Ice_Melt_Rate_East\": [ice_melt_rate_east],\n",
        "                    \"Ice_Melt_Rate_West\": [ice_melt_rate_west],\n",
        "                    \"SST_East\": [df_sst_east[\"Sea_Surface_Temp_C_East\"].iloc[0]],\n",
        "                    \"SST_West\": [df_sst_west[\"Sea_Surface_Temp_C_West\"].iloc[0]]\n",
        "                })\n",
        "\n",
        "                # Validate\n",
        "                dtypes = {\"Year\": int, \"Quarter\": str, \"Ice_Melt_Rate_East\": float, \"Ice_Melt_Rate_West\": float, \"SST_East\": float, \"SST_West\": float}\n",
        "                validate_dataframe(df_new, expected_columns, dtypes)\n",
        "\n",
        "                # Insert directly into database\n",
        "                df_new.to_sql('ice_melt_sst', conn, if_exists='append', index=False)\n",
        "                print(\"Inserted new data into ice_melt_sst table\")\n",
        "\n",
        "                # Generate DML statements\n",
        "                dml_statements = []\n",
        "                for _, row in df_new.iterrows():\n",
        "                    dml_statements.append(\n",
        "                        f\"INSERT INTO ice_melt_sst (Year, Quarter, Ice_Melt_Rate_East, Ice_Melt_Rate_West, SST_East, SST_West) VALUES \"\n",
        "                        f\"({row['Year']}, '{row['Quarter']}', {row['Ice_Melt_Rate_East']}, {row['Ice_Melt_Rate_West']}, {row['SST_East']}, {row['SST_West']})\"\n",
        "                    )\n",
        "\n",
        "                # Append DML statements\n",
        "                with open(DML_PATH, 'a') as f:\n",
        "                    f.write(\"\\n-- Update for ice_melt_sst\\n\")\n",
        "                    f.write(\"\\n\".join(dml_statements) + \";\\n\")\n",
        "\n",
        "                print(\"Updated ice_melt_sst table with new data\")\n",
        "            else:\n",
        "                print(\"No new data available for ice_melt_sst (requires updated sst_east and sst_west data)\")\n",
        "        else:\n",
        "            print(f\"Data for {next_year} {next_quarter} already exists in ice_melt_sst, skipping update.\")\n",
        "    else:\n",
        "        print(\"Skipping ice_melt_sst update as sst_east or sst_west was not updated.\")\n",
        "\n",
        "# Display updated data\n",
        "print(\"Updated Ice Melt SST DataFrame:\")\n",
        "df_updated = pd.read_sql_query(\"SELECT * FROM ice_melt_sst WHERE Year = ? AND Quarter = ?\", conn, params=(next_year, next_quarter))\n",
        "display(df_updated)"
      ],
      "metadata": {
        "id": "fefe0sgOxKl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Close database connection\n",
        "conn.commit()\n",
        "conn.close()\n",
        "print(\"Database connection closed and updates saved to /content/drive/MyDrive/greenland_fishery.db\")"
      ],
      "metadata": {
        "id": "vu-0riJCVuEJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}