{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2oC5u_6RXbb"
      },
      "outputs": [],
      "source": [
        "!pip install requests xarray pandas pyjstat datetime pydap netCDF4\n",
        "import os\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import requests\n",
        "from pyjstat import pyjstat\n",
        "from urllib.parse import quote\n",
        "from io import StringIO\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "\n",
        "!apt-get update && apt-get install -y iputils-ping"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a SQLite database\n",
        "db_path = 'greenland_fishery.db'\n",
        "conn = sqlite3.connect(db_path)\n",
        "cursor = conn.cursor()\n",
        "print(f\"Connected to SQLite database at {db_path}\")"
      ],
      "metadata": {
        "id": "gwne4pS6RwUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD VAR 1 TOTAL CATCH\n",
        "\n",
        "df_clean = None\n",
        "\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='total_catch'\")\n",
        "if cursor.fetchone():\n",
        "    print(\"Loading Total Catch from SQLite database\")\n",
        "    df_clean = pd.read_sql_query(\"SELECT * FROM total_catch\", conn)\n",
        "    print(\"Loaded DataFrame from SQLite:\")\n",
        "    # Validate the loaded data\n",
        "    if df_clean['Quarter'].isnull().any():\n",
        "        print(\"Warning: Found NaN values in Quarter column. Dropping and recreating table...\")\n",
        "        cursor.execute(\"DROP TABLE total_catch\")\n",
        "        conn.commit()\n",
        "    else:\n",
        "        print(\"Loaded data is valid.\")\n",
        "else:\n",
        "    print(\"total_catch table not found, querying API and creating table...\")\n",
        "    url = \"https://bank.stat.gl:443/api/v1/en/Greenland/FI/FI10/FIX008.px\"\n",
        "    query = {\n",
        "        \"query\": [\n",
        "            {\"code\": \"nation\", \"selection\": {\"filter\": \"item\", \"values\": [\"GRL\"]}},\n",
        "            {\"code\": \"unit\", \"selection\": {\"filter\": \"item\", \"values\": [\"Ton\"]}},\n",
        "            {\"code\": \"time\", \"selection\": {\"filter\": \"item\", \"values\": [str(y) for y in range(2011, 2025)]}},\n",
        "            {\"code\": \"quarter\", \"selection\": {\"filter\": \"item\", \"values\": [\"1\", \"2\", \"3\", \"4\"]}}\n",
        "        ],\n",
        "        \"response\": {\"format\": \"json-stat2\"}\n",
        "    }\n",
        "    response = requests.post(url, json=query)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        dataset = pyjstat.Dataset.read(response.text)\n",
        "        df = dataset.write('dataframe')\n",
        "        print(\"Data successfully retrieved and converted to DataFrame!\")\n",
        "\n",
        "        # Clean DataFrame\n",
        "        df_clean = df.copy()\n",
        "        df_clean.drop(columns=['nation'], inplace=True)\n",
        "        df_clean.rename(columns={\n",
        "            \"time\": \"Year\",\n",
        "            \"quarter\": \"Quarter\",\n",
        "            \"unit\": \"Unit\",\n",
        "            \"value\": \"Total_Catch\"\n",
        "        }, inplace=True)\n",
        "        df_clean[\"Quarter\"] = df_clean[\"Quarter\"].str.replace(\"Quarter \", \"Q\")\n",
        "        quarter_order = [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]\n",
        "        df_clean[\"Quarter\"] = pd.Categorical(df_clean[\"Quarter\"], categories=quarter_order, ordered=True)\n",
        "        df_clean = df_clean[[\"Year\", \"Quarter\", \"Unit\", \"Total_Catch\"]]\n",
        "        df_clean[\"Year\"] = df_clean[\"Year\"].astype(int)\n",
        "\n",
        "        # Validate before saving to SQLite\n",
        "        if df_clean.isnull().any().any():\n",
        "            raise ValueError(\"DataFrame contains NaN values before saving to SQLite: \" + str(df_clean.head()))\n",
        "\n",
        "        # Create table\n",
        "        cursor.execute(\"\"\"\n",
        "            CREATE TABLE total_catch (\n",
        "                Year INTEGER,\n",
        "                Quarter TEXT,\n",
        "                Unit TEXT,\n",
        "                Total_Catch INTEGER\n",
        "            )\n",
        "        \"\"\")\n",
        "\n",
        "        # Insert data into table\n",
        "        df_clean.to_sql('total_catch', conn, if_exists='append', index=False)\n",
        "        conn.commit()\n",
        "        print(\"Saved Total Catch to SQLite table 'total_catch'\")\n",
        "    else:\n",
        "        print(f\"Error {response.status_code}: {response.text}\")\n",
        "\n",
        "# Final display\n",
        "print(\"Final Total Catch DataFrame:\")\n",
        "display(df_clean.head())"
      ],
      "metadata": {
        "id": "yPmODF9IRzxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Variable 2: Exports of Fish\n",
        "\n",
        "df_fish_clean = None\n",
        "\n",
        "# Check if fish_exports table exists\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='fish_exports'\")\n",
        "if cursor.fetchone():\n",
        "    print(\"Loading Fish Exports from SQLite database\")\n",
        "    df_fish_clean = pd.read_sql_query(\"SELECT * FROM fish_exports\", conn)\n",
        "    print(\"Loaded DataFrame from SQLite:\")\n",
        "    if df_fish_clean['Quarter'].isnull().any():\n",
        "        print(\"Warning: Found NaN values in Quarter column. Dropping and recreating table...\")\n",
        "        cursor.execute(\"DROP TABLE fish_exports\")\n",
        "        conn.commit()\n",
        "        df_fish_clean = None\n",
        "    else:\n",
        "        print(\"Loaded data is valid.\")\n",
        "else:\n",
        "    print(\"fish_exports table not found, querying API and creating table...\")\n",
        "    url = \"https://bank.stat.gl:443/api/v1/en/Greenland/IE/IE10/IEX2PROD.px\"\n",
        "    query = {\n",
        "        \"query\": [\n",
        "            {\"code\": \"branch\", \"selection\": {\"filter\": \"item\", \"values\": [\"46\"]}},\n",
        "            {\"code\": \"quarter\", \"selection\": {\"filter\": \"item\", \"values\": [\"1\", \"2\", \"3\", \"4\"]}},\n",
        "            {\"code\": \"time\", \"selection\": {\"filter\": \"item\", \"values\": [str(y) for y in range(2011, 2025)]}}\n",
        "        ],\n",
        "        \"response\": {\"format\": \"json-stat2\"}\n",
        "    }\n",
        "    try:\n",
        "        response = requests.post(url, json=query)\n",
        "        response.raise_for_status()\n",
        "        print(\"API Response:\")\n",
        "        print(response.text[:500])\n",
        "\n",
        "        dataset = pyjstat.Dataset.read(response.text)\n",
        "        df_fish_exports = dataset.write('dataframe')\n",
        "        print(\"Fish export data successfully retrieved!\")\n",
        "\n",
        "        # Clean DataFrame\n",
        "        df_fish_clean = df_fish_exports.copy()\n",
        "        column_mapping = {\n",
        "            \"time\": \"Year\",\n",
        "            \"quarter\": \"Quarter\",\n",
        "            \"value\": \"Fish_Export_Value_Million_Kr\"\n",
        "        }\n",
        "        if \"quarter\" not in df_fish_exports.columns and \"Quarter\" in df_fish_exports.columns:\n",
        "            column_mapping[\"Quarter\"] = \"Quarter\"\n",
        "            del column_mapping[\"quarter\"]\n",
        "        df_fish_clean.rename(columns=column_mapping, inplace=True)\n",
        "\n",
        "        if \"Quarter\" not in df_fish_clean.columns:\n",
        "            raise ValueError(\"Quarter column missing after renaming.\")\n",
        "        if df_fish_clean[\"Quarter\"].isnull().any():\n",
        "            raise ValueError(\"Quarter column contains NaN values after renaming: \" + str(df_fish_clean[\"Quarter\"].head()))\n",
        "\n",
        "        # Fix quarters (case-insensitive replacement)\n",
        "        df_fish_clean[\"Quarter\"] = df_fish_clean[\"Quarter\"].str.replace(r\"[Qq]uarter \", \"Q\", regex=True)\n",
        "        quarter_order = [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]\n",
        "        df_fish_clean[\"Quarter\"] = pd.Categorical(df_fish_clean[\"Quarter\"], categories=quarter_order, ordered=True)\n",
        "        df_fish_clean = df_fish_clean.sort_values(by=[\"Year\", \"Quarter\"]).reset_index(drop=True)\n",
        "        print(\"After fixing quarters:\")\n",
        "        display(df_fish_clean.head())\n",
        "\n",
        "        if df_fish_clean[\"Quarter\"].isnull().any():\n",
        "            raise ValueError(\"Quarter column contains NaN values after transformation: \" + str(df_fish_clean[\"Quarter\"].head()))\n",
        "\n",
        "        # Convert export value to million Kr and round\n",
        "        df_fish_clean[\"Fish_Export_Value_Million_Kr\"] = df_fish_clean[\"Fish_Export_Value_Million_Kr\"] / 1e6\n",
        "        df_fish_clean[\"Fish_Export_Value_Million_Kr\"] = df_fish_clean[\"Fish_Export_Value_Million_Kr\"].round(0).astype(int)\n",
        "        df_fish_clean = df_fish_clean[[\"Year\", \"Quarter\", \"Fish_Export_Value_Million_Kr\"]]\n",
        "        df_fish_clean[\"Year\"] = df_fish_clean[\"Year\"].astype(int)\n",
        "\n",
        "        if df_fish_clean.isnull().any().any():\n",
        "            raise ValueError(\"DataFrame contains NaN values before saving to SQLite.\")\n",
        "\n",
        "        # Create table\n",
        "        cursor.execute(\"\"\"\n",
        "            CREATE TABLE fish_exports (\n",
        "                Year INTEGER,\n",
        "                Quarter TEXT,\n",
        "                Fish_Export_Value_Million_Kr INTEGER\n",
        "            )\n",
        "        \"\"\")\n",
        "\n",
        "        # Insert data into table\n",
        "        df_fish_clean.to_sql('fish_exports', conn, if_exists='append', index=False)\n",
        "        conn.commit()\n",
        "        print(\"Saved Fish Exports to SQLite table 'fish_exports'\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Failed to retrieve data from API: {e}\")\n",
        "\n",
        "# Final display\n",
        "print(\"Final Fish Exports DataFrame:\")\n",
        "if df_fish_clean is not None:\n",
        "    display(df_fish_clean.head())\n",
        "else:\n",
        "    print(\"Error: df_fish_clean not created due to API failure.\")"
      ],
      "metadata": {
        "id": "V_sLw7SyR2B7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VARIABLE 3: WEST GREENLAND SST\n",
        "\n",
        "df_sst_west_clean = None\n",
        "\n",
        "# Degree to ERDDAP grid index conversion\n",
        "def deg_to_index_lat(lat): return int(round((lat + 90) / 0.25))\n",
        "def deg_to_index_lon(lon): return int(round((lon + 180) / 0.25))\n",
        "\n",
        "# Define bounding box in degrees\n",
        "bbox_deg = {\n",
        "    'lat_min': 65.0,\n",
        "    'lat_max': 70.0,\n",
        "    'lon_min': -55.0,\n",
        "    'lon_max': -50.0\n",
        "}\n",
        "\n",
        "# Convert to grid indices\n",
        "bbox_idx = {\n",
        "    'lat_min': deg_to_index_lat(bbox_deg['lat_min']),\n",
        "    'lat_max': deg_to_index_lat(bbox_deg['lat_max']),\n",
        "    'lon_min': deg_to_index_lon(bbox_deg['lon_min']),\n",
        "    'lon_max': deg_to_index_lon(bbox_deg['lon_max'])\n",
        "}\n",
        "print(\"Bounding box indices:\", bbox_idx)\n",
        "\n",
        "# Check if sst_west table exists\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='sst_west'\")\n",
        "if cursor.fetchone():\n",
        "    print(\"Loading SST West from SQLite database\")\n",
        "    df_sst_west_clean = pd.read_sql_query(\"SELECT * FROM sst_west\", conn)\n",
        "    print(\"Loaded data is valid.\")\n",
        "else:\n",
        "    print(\"sst_west table not found, querying API and creating table...\")\n",
        "    years = list(range(2011, 2025))\n",
        "    west_quarters = []\n",
        "    for year in years:\n",
        "        print(f\"Processing year: {year}\")\n",
        "        try:\n",
        "            base = \"https://coastwatch.pfeg.noaa.gov/erddap/griddap/ncdcOisst21Agg_LonPM180.csv?\"\n",
        "            var = \"sst\"\n",
        "            time = f\"[({year}-01-01T00:00:00Z):1:({year}-12-31T00:00:00Z)]\".replace(\" \", \"\")\n",
        "            zlev = \"[0:1:0]\"\n",
        "            lat = f\"[({bbox_idx['lat_min']}):1:({bbox_idx['lat_max']})]\"\n",
        "            lon = f\"[({bbox_idx['lon_min']}):1:({bbox_idx['lon_max']})]\"\n",
        "            query = f\"{var}{time}{zlev}{lat}{lon}\"\n",
        "            full_url = base + quote(query, safe=\":/[](),-T\")\n",
        "            print(\"Constructed URL:\", full_url)\n",
        "\n",
        "            response = requests.get(full_url)\n",
        "            if response.status_code != 200:\n",
        "                raise ValueError(f\"HTTP {response.status_code}: {response.text}\")\n",
        "\n",
        "            df = pd.read_csv(StringIO(response.text), skiprows=[1])\n",
        "            df = df.rename(columns={col: col.strip() for col in df.columns})\n",
        "            df = df.dropna(subset=[\"sst\"])\n",
        "\n",
        "            df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
        "            df[\"Year\"] = df[\"time\"].dt.year.astype(int)\n",
        "            df[\"Quarter\"] = \"Q\" + df[\"time\"].dt.quarter.astype(str)\n",
        "\n",
        "            df_q = df.groupby(['Year', 'Quarter'])[\"sst\"].mean().reset_index()\n",
        "            df_q = df_q.rename(columns={\"sst\": \"Sea_Surface_Temp_C_West\"})\n",
        "\n",
        "            df_q[\"Melt_Active_West\"] = (df_q[\"Sea_Surface_Temp_C_West\"] > 0.5).astype(int)\n",
        "            df_q[\"Melt_Index_West\"] = df_q[\"Sea_Surface_Temp_C_West\"].clip(lower=0, upper=4) / 4\n",
        "\n",
        "            west_quarters.append(df_q)\n",
        "            print(f\"{year} processed.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed for {year}: {e}\")\n",
        "\n",
        "    if west_quarters:\n",
        "        df_sst_west_clean = pd.concat(west_quarters).reset_index(drop=True)\n",
        "        if df_sst_west_clean.isnull().any().any():\n",
        "            raise ValueError(\"DataFrame contains NaN values before saving to SQLite.\")\n",
        "\n",
        "        cursor.execute(\"\"\"\n",
        "            CREATE TABLE sst_west (\n",
        "                Year INTEGER,\n",
        "                Quarter TEXT,\n",
        "                Sea_Surface_Temp_C_West REAL,\n",
        "                Melt_Active_West INTEGER,\n",
        "                Melt_Index_West REAL\n",
        "            )\n",
        "        \"\"\")\n",
        "        df_sst_west_clean.to_sql('sst_west', conn, if_exists='append', index=False)\n",
        "        conn.commit()\n",
        "        print(\"Saved SST West to SQLite table 'sst_west'\")\n",
        "    else:\n",
        "        print(\"No data retrieved for SST West.\")\n",
        "\n",
        "# Final display\n",
        "print(\"Final SST West DataFrame:\")\n",
        "if df_sst_west_clean is not None:\n",
        "    print(\"Final SST dataset shape:\", df_sst_west_clean.shape)\n",
        "    display(df_sst_west_clean.head())\n",
        "else:\n",
        "    print(\"Error: df_sst_west_clean not created due to API failure.\")"
      ],
      "metadata": {
        "id": "TOcaW3MuR2fG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable 4: East Greenland SST, Melt\n",
        "\n",
        "df_sst_east_clean = None\n",
        "\n",
        "# Define bounding box in degrees\n",
        "bbox_deg_east = {\n",
        "    'lat_min': 65.0,\n",
        "    'lat_max': 70.0,\n",
        "    'lon_min': -40.0,\n",
        "    'lon_max': -35.0\n",
        "}\n",
        "\n",
        "# Convert to grid indices (using same functions as SST West)\n",
        "bbox_idx_east = {\n",
        "    'lat_min': deg_to_index_lat(bbox_deg_east['lat_min']),\n",
        "    'lat_max': deg_to_index_lat(bbox_deg_east['lat_max']),\n",
        "    'lon_min': deg_to_index_lon(bbox_deg_east['lon_min']),\n",
        "    'lon_max': deg_to_index_lon(bbox_deg_east['lon_max'])\n",
        "}\n",
        "print(\"East Greenland bounding box indices:\", bbox_idx_east)\n",
        "\n",
        "# Check if sst_east table exists\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='sst_east'\")\n",
        "if cursor.fetchone():\n",
        "    print(\"Loading SST East from SQLite database\")\n",
        "    df_sst_east_clean = pd.read_sql_query(\"SELECT * FROM sst_east\", conn)\n",
        "    print(\"Loaded data is valid.\")\n",
        "else:\n",
        "    print(\"sst_east table not found, querying API and creating table...\")\n",
        "    years = list(range(2011, 2025))\n",
        "    east_quarters = []\n",
        "    for year in years:\n",
        "        print(f\"Processing East Greenland year: {year}\")\n",
        "        try:\n",
        "            base = \"https://coastwatch.pfeg.noaa.gov/erddap/griddap/ncdcOisst21Agg_LonPM180.csv?\"\n",
        "            var = \"sst\"\n",
        "            time = f\"[({year}-01-01T00:00:00Z):1:({year}-12-31T00:00:00Z)]\".replace(\" \", \"\")\n",
        "            zlev = \"[0:1:0]\"\n",
        "            lat = f\"[({bbox_idx_east['lat_min']}):1:({bbox_idx_east['lat_max']})]\"\n",
        "            lon = f\"[({bbox_idx_east['lon_min']}):1:({bbox_idx_east['lon_max']})]\"\n",
        "            query = f\"{var}{time}{zlev}{lat}{lon}\"\n",
        "            full_url = base + quote(query, safe=\":/[](),-\")\n",
        "            print(\"Constructed URL:\", full_url)\n",
        "            response = requests.get(full_url)\n",
        "            if response.status_code != 200:\n",
        "                raise ValueError(f\"HTTP {response.status_code}: {response.text}\")\n",
        "            df = pd.read_csv(StringIO(response.text), skiprows=[1])\n",
        "            df = df.rename(columns={col: col.strip() for col in df.columns})\n",
        "            df = df.dropna(subset=[\"sst\"])\n",
        "            df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
        "            df[\"Year\"] = df[\"time\"].dt.year.astype(int)\n",
        "            df[\"Quarter\"] = \"Q\" + df[\"time\"].dt.quarter.astype(str)\n",
        "            df_q = df.groupby(['Year', 'Quarter'])[\"sst\"].mean().reset_index()\n",
        "            df_q = df_q.rename(columns={\"sst\": \"Sea_Surface_Temp_C_East\"})\n",
        "            df_q[\"Melt_Active_East\"] = (df_q[\"Sea_Surface_Temp_C_East\"] > 0.5).astype(int)\n",
        "            df_q[\"Melt_Index_East\"] = df_q[\"Sea_Surface_Temp_C_East\"].clip(lower=0, upper=4) / 4\n",
        "            east_quarters.append(df_q)\n",
        "            print(f\"{year} processed.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed for {year}: {e}\")\n",
        "\n",
        "    if east_quarters:\n",
        "        df_sst_east_clean = pd.concat(east_quarters).reset_index(drop=True)\n",
        "        if df_sst_east_clean.isnull().any().any():\n",
        "            raise ValueError(\"DataFrame contains NaN values before saving to SQLite.\")\n",
        "\n",
        "        cursor.execute(\"\"\"\n",
        "            CREATE TABLE sst_east (\n",
        "                Year INTEGER,\n",
        "                Quarter TEXT,\n",
        "                Sea_Surface_Temp_C_East REAL,\n",
        "                Melt_Active_East INTEGER,\n",
        "                Melt_Index_East REAL\n",
        "            )\n",
        "        \"\"\")\n",
        "        df_sst_east_clean.to_sql('sst_east', conn, if_exists='append', index=False)\n",
        "        conn.commit()\n",
        "        print(\"Saved SST East to SQLite table 'sst_east'\")\n",
        "    else:\n",
        "        print(\"No data retrieved for SST East.\")\n",
        "\n",
        "# Final display\n",
        "print(\"Final SST East DataFrame:\")\n",
        "if df_sst_east_clean is not None:\n",
        "    print(\"Final SST East dataset shape:\", df_sst_east_clean.shape)\n",
        "    display(df_sst_east_clean.head())\n",
        "else:\n",
        "    print(\"Error: df_sst_east_clean not created due to API failure.\")"
      ],
      "metadata": {
        "id": "1nDw7roWSZlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable 5: South Greenland SST, Melt\n",
        "\n",
        "df_sst_south_clean = None\n",
        "\n",
        "# Define bounding box in degrees\n",
        "bbox_deg_south = {\n",
        "    'lat_min': 60.0,\n",
        "    'lat_max': 65.0,\n",
        "    'lon_min': -45.0,\n",
        "    'lon_max': -40.0\n",
        "}\n",
        "\n",
        "# Convert to grid indices\n",
        "bbox_idx_south = {\n",
        "    'lat_min': deg_to_index_lat(bbox_deg_south['lat_min']),\n",
        "    'lat_max': deg_to_index_lat(bbox_deg_south['lat_max']),\n",
        "    'lon_min': deg_to_index_lon(bbox_deg_south['lon_min']),\n",
        "    'lon_max': deg_to_index_lon(bbox_deg_south['lon_max'])\n",
        "}\n",
        "print(\"South Greenland bounding box indices:\", bbox_idx_south)\n",
        "\n",
        "# Check if sst_south table exists\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='sst_south'\")\n",
        "if cursor.fetchone():\n",
        "    print(\"Loading SST South from SQLite database\")\n",
        "    df_sst_south_clean = pd.read_sql_query(\"SELECT * FROM sst_south\", conn)\n",
        "    print(\"Loaded data is valid.\")\n",
        "else:\n",
        "    print(\"sst_south table not found, querying API and creating table...\")\n",
        "    years = list(range(2011, 2025))\n",
        "    south_quarters = []\n",
        "    for year in years:\n",
        "        print(f\"Processing South Greenland year: {year}\")\n",
        "        try:\n",
        "            base = \"https://coastwatch.pfeg.noaa.gov/erddap/griddap/ncdcOisst21Agg_LonPM180.csv?\"\n",
        "            var = \"sst\"\n",
        "            time = f\"[({year}-01-01T00:00:00Z):1:({year}-12-31T00:00:00Z)]\".replace(\" \", \"\")\n",
        "            zlev = \"[0:1:0]\"\n",
        "            lat = f\"[({bbox_idx_south['lat_min']}):1:({bbox_idx_south['lat_max']})]\"\n",
        "            lon = f\"[({bbox_idx_south['lon_min']}):1:({bbox_idx_south['lon_max']})]\"\n",
        "            query = f\"{var}{time}{zlev}{lat}{lon}\"\n",
        "            full_url = base + quote(query, safe=\":/[](),-\")\n",
        "            print(\"Constructed URL:\", full_url)\n",
        "            response = requests.get(full_url)\n",
        "            if response.status_code != 200:\n",
        "                raise ValueError(f\"HTTP {response.status_code}: {response.text}\")\n",
        "            df = pd.read_csv(StringIO(response.text), skiprows=[1])\n",
        "            df = df.rename(columns={col: col.strip() for col in df.columns})\n",
        "            df = df.dropna(subset=[\"sst\"])\n",
        "            df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
        "            df[\"Year\"] = df[\"time\"].dt.year.astype(int)\n",
        "            df[\"Quarter\"] = \"Q\" + df[\"time\"].dt.quarter.astype(str)\n",
        "            df_q = df.groupby(['Year', 'Quarter'])[\"sst\"].mean().reset_index()\n",
        "            df_q = df_q.rename(columns={\"sst\": \"Sea_Surface_Temp_C_South\"})\n",
        "            df_q[\"Melt_Active_South\"] = (df_q[\"Sea_Surface_Temp_C_South\"] > 0.5).astype(int)\n",
        "            df_q[\"Melt_Index_South\"] = df_q[\"Sea_Surface_Temp_C_South\"].clip(lower=0, upper=4) / 4\n",
        "            south_quarters.append(df_q)\n",
        "            print(f\"{year} processed.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed for {year}: {e}\")\n",
        "\n",
        "    if south_quarters:\n",
        "        df_sst_south_clean = pd.concat(south_quarters).reset_index(drop=True)\n",
        "        if df_sst_south_clean.isnull().any().any():\n",
        "            raise ValueError(\"DataFrame contains NaN values before saving to SQLite.\")\n",
        "\n",
        "        cursor.execute(\"\"\"\n",
        "            CREATE TABLE sst_south (\n",
        "                Year INTEGER,\n",
        "                Quarter TEXT,\n",
        "                Sea_Surface_Temp_C_South REAL,\n",
        "                Melt_Active_South INTEGER,\n",
        "                Melt_Index_South REAL\n",
        "            )\n",
        "        \"\"\")\n",
        "        df_sst_south_clean.to_sql('sst_south', conn, if_exists='append', index=False)\n",
        "        conn.commit()\n",
        "        print(\"Saved SST South to SQLite table 'sst_south'\")\n",
        "    else:\n",
        "        print(\"No data retrieved for SST South.\")\n",
        "\n",
        "# Final display\n",
        "print(\"Final SST South DataFrame:\")\n",
        "if df_sst_south_clean is not None:\n",
        "    print(\"Final SST South dataset shape:\", df_sst_south_clean.shape)\n",
        "    display(df_sst_south_clean.head())\n",
        "else:\n",
        "    print(\"Error: df_sst_south_clean not created due to API failure.\")"
      ],
      "metadata": {
        "id": "nMfok7CfTENF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Variable 6: Total Catch by Foreign Vessels\n",
        "\n",
        "df_foreign_clean = None\n",
        "\n",
        "# Check if foreign_catch table exists\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='foreign_catch'\")\n",
        "if cursor.fetchone():\n",
        "    print(\"Loading Foreign Catch from SQLite database\")\n",
        "    df_foreign_clean = pd.read_sql_query(\"SELECT * FROM foreign_catch\", conn)\n",
        "    print(\"Loaded data is valid.\")\n",
        "else:\n",
        "    print(\"foreign_catch table not found, querying API and creating table...\")\n",
        "    url = \"https://bank.stat.gl:443/api/v1/en/Greenland/FI/FI10/FIX008.px\"\n",
        "    query = {\n",
        "        \"query\": [\n",
        "            {\"code\": \"nation\", \"selection\": {\"filter\": \"item\", \"values\": [\"UDL\"]}},\n",
        "            {\"code\": \"unit\", \"selection\": {\"filter\": \"item\", \"values\": [\"Ton\"]}},\n",
        "            {\"code\": \"time\", \"selection\": {\"filter\": \"item\", \"values\": [str(y) for y in range(2011, 2025)]}},\n",
        "            {\"code\": \"quarter\", \"selection\": {\"filter\": \"item\", \"values\": [\"1\", \"2\", \"3\", \"4\"]}}\n",
        "        ],\n",
        "        \"response\": {\"format\": \"json-stat2\"}\n",
        "    }\n",
        "    try:\n",
        "        response = requests.post(url, json=query)\n",
        "        if response.status_code == 200:\n",
        "            dataset = pyjstat.Dataset.read(response.text)\n",
        "            df = dataset.write('dataframe')\n",
        "            print(\"Foreign vessel data successfully retrieved and converted to DataFrame!\")\n",
        "\n",
        "            # Clean DataFrame\n",
        "            df_foreign_clean = df.copy()\n",
        "            df_foreign_clean.drop(columns=[\"nation\"], inplace=True)\n",
        "            df_foreign_clean.rename(columns={\n",
        "                \"time\": \"Year\",\n",
        "                \"quarter\": \"Quarter\",\n",
        "                \"unit\": \"Unit\",\n",
        "                \"value\": \"Foreign_Catch\"\n",
        "            }, inplace=True)\n",
        "            df_foreign_clean[\"Quarter\"] = df_foreign_clean[\"Quarter\"].str.replace(\"Quarter \", \"Q\")\n",
        "            quarter_order = [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]\n",
        "            df_foreign_clean[\"Quarter\"] = pd.Categorical(df_foreign_clean[\"Quarter\"], categories=quarter_order, ordered=True)\n",
        "            df_foreign_clean = df_foreign_clean.sort_values(by=[\"Year\", \"Quarter\"]).reset_index(drop=True)\n",
        "            df_foreign_clean = df_foreign_clean[[\"Year\", \"Quarter\", \"Unit\", \"Foreign_Catch\"]]\n",
        "            df_foreign_clean[\"Year\"] = df_foreign_clean[\"Year\"].astype(int)\n",
        "\n",
        "            if df_foreign_clean.isnull().any().any():\n",
        "                raise ValueError(\"DataFrame contains NaN values before saving to SQLite.\")\n",
        "\n",
        "            # Create table\n",
        "            cursor.execute(\"\"\"\n",
        "                CREATE TABLE foreign_catch (\n",
        "                    Year INTEGER,\n",
        "                    Quarter TEXT,\n",
        "                    Unit TEXT,\n",
        "                    Foreign_Catch INTEGER\n",
        "                )\n",
        "            \"\"\")\n",
        "            df_foreign_clean.to_sql('foreign_catch', conn, if_exists='append', index=False)\n",
        "            conn.commit()\n",
        "            print(\"Saved Foreign Catch to SQLite table 'foreign_catch'\")\n",
        "        else:\n",
        "            print(f\"Error {response.status_code}: {response.text}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to retrieve data from API: {e}\")\n",
        "\n",
        "# Final display\n",
        "print(\"Final Foreign Catch DataFrame:\")\n",
        "if df_foreign_clean is not None:\n",
        "    display(df_foreign_clean.head())\n",
        "else:\n",
        "    print(\"Error: df_foreign_clean not created due to API failure.\")"
      ],
      "metadata": {
        "id": "N4TVxXJUSjZv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}