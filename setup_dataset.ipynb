{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkKLKYwbf1+uYS9OIwxJti",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccspen21/greenland-fishery-nowcast-2025/blob/main/setup_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2oC5u_6RXbb"
      },
      "outputs": [],
      "source": [
        "!pip install requests xarray pandas pyjstat datetime pydap netCDF4\n",
        "import os\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import requests\n",
        "from pyjstat import pyjstat\n",
        "from urllib.parse import quote\n",
        "from io import StringIO\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "# Ensure compatibility with Colab and GitHub\n",
        "!apt-get update && apt-get install -y iputils-ping\n",
        "\n",
        "# Define a configurable database path\n",
        "DB_PATH = os.getenv(\"DB_PATH\", \"greenland_fishery.db\")  # Use environment variable or default to local file"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to the SQLite database\n",
        "conn = sqlite3.connect(DB_PATH)\n",
        "cursor = conn.cursor()\n",
        "print(f\"Connected to SQLite database at {DB_PATH}\")\n",
        "\n",
        "# Load and execute the DDL script to create the database schema\n",
        "def execute_sql_script(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            sql_script = file.read()\n",
        "        cursor.executescript(sql_script)\n",
        "        conn.commit()\n",
        "        print(f\"Successfully executed SQL script: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error executing SQL script {file_path}: {e}\")\n",
        "        raise\n",
        "\n",
        "# Execute the DDL script\n",
        "execute_sql_script('ddl_schema.sql')"
      ],
      "metadata": {
        "id": "gwne4pS6RwUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD VAR 1 TOTAL CATCH\n",
        "# LOAD VAR 1 TOTAL CATCH\n",
        "\n",
        "# LOAD VAR 1 TOTAL CATCH\n",
        "\n",
        "df_clean = None\n",
        "\n",
        "# Helper function to validate DataFrame against schema\n",
        "def validate_dataframe(df, expected_columns, dtypes):\n",
        "    if not all(col in df.columns for col in expected_columns):\n",
        "        raise ValueError(f\"DataFrame missing expected columns: {expected_columns}\")\n",
        "    for col, dtype in dtypes.items():\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].astype(dtype)\n",
        "    if df.isnull().any().any():\n",
        "        raise ValueError(f\"DataFrame contains NaN values: {df.head()}\")\n",
        "\n",
        "# Check if data exists in the database\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='total_catch'\")\n",
        "if cursor.fetchone():\n",
        "    print(\"Loading Total Catch from SQLite database\")\n",
        "    df_clean = pd.read_sql_query(\"SELECT * FROM total_catch\", conn)\n",
        "    print(\"Loaded DataFrame from SQLite:\")\n",
        "    expected_columns = [\"Year\", \"Quarter\", \"Unit\", \"Total_Catch\"]\n",
        "    dtypes = {\"Year\": int, \"Quarter\": str, \"Unit\": str, \"Total_Catch\": int}\n",
        "    try:\n",
        "        validate_dataframe(df_clean, expected_columns, dtypes)\n",
        "        print(\"Loaded data is valid.\")\n",
        "    except ValueError as e:\n",
        "        print(f\"Validation error: {e}. Dropping and recreating table...\")\n",
        "        cursor.execute(\"DROP TABLE total_catch\")\n",
        "        conn.commit()\n",
        "        execute_sql_script('ddl_schema.sql')  # Recreate the schema\n",
        "        df_clean = None\n",
        "\n",
        "if df_clean is None:\n",
        "    print(\"total_catch table not found or invalid, querying API...\")\n",
        "    url = \"https://bank.stat.gl:443/api/v1/en/Greenland/FI/FI10/FIX008.px\"\n",
        "    query = {\n",
        "        \"query\": [\n",
        "            {\"code\": \"nation\", \"selection\": {\"filter\": \"item\", \"values\": [\"GRL\"]}},\n",
        "            {\"code\": \"unit\", \"selection\": {\"filter\": \"item\", \"values\": [\"Ton\"]}},\n",
        "            {\"code\": \"time\", \"selection\": {\"filter\": \"item\", \"values\": [str(y) for y in range(2011, 2025)]}},\n",
        "            {\"code\": \"quarter\", \"selection\": {\"filter\": \"item\", \"values\": [\"1\", \"2\", \"3\", \"4\"]}}\n",
        "        ],\n",
        "        \"response\": {\"format\": \"json-stat2\"}\n",
        "    }\n",
        "    try:\n",
        "        response = requests.post(url, json=query, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        dataset = pyjstat.Dataset.read(response.text)\n",
        "        df = dataset.write('dataframe')\n",
        "        print(\"Data successfully retrieved and converted to DataFrame!\")\n",
        "\n",
        "        # Clean DataFrame\n",
        "        df_clean = df.copy()\n",
        "        df_clean.drop(columns=['nation'], inplace=True)\n",
        "        df_clean.rename(columns={\n",
        "            \"time\": \"Year\",\n",
        "            \"quarter\": \"Quarter\",\n",
        "            \"unit\": \"Unit\",\n",
        "            \"value\": \"Total_Catch\"\n",
        "        }, inplace=True)\n",
        "        df_clean[\"Quarter\"] = df_clean[\"Quarter\"].str.replace(\"Quarter \", \"Q\")\n",
        "        quarter_order = [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]\n",
        "        df_clean[\"Quarter\"] = pd.Categorical(df_clean[\"Quarter\"], categories=quarter_order, ordered=True)\n",
        "        df_clean = df_clean[[\"Year\", \"Quarter\", \"Unit\", \"Total_Catch\"]]\n",
        "        df_clean[\"Year\"] = df_clean[\"Year\"].astype(int)\n",
        "\n",
        "        # Validate before saving to SQLite\n",
        "        expected_columns = [\"Year\", \"Quarter\", \"Unit\", \"Total_Catch\"]\n",
        "        dtypes = {\"Year\": int, \"Quarter\": str, \"Unit\": str, \"Total_Catch\": int}\n",
        "        validate_dataframe(df_clean, expected_columns, dtypes)\n",
        "\n",
        "        # Insert data into table (schema already created)\n",
        "        df_clean.to_sql('total_catch', conn, if_exists='append', index=False)\n",
        "        conn.commit()\n",
        "        print(\"Saved Total Catch to SQLite table 'total_catch'\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching Total Catch data: {e}\")\n",
        "        df_clean = pd.DataFrame(columns=[\"Year\", \"Quarter\", \"Unit\", \"Total_Catch\"])  # Empty DataFrame as fallback\n",
        "\n",
        "# Final display\n",
        "print(\"Final Total Catch DataFrame:\")\n",
        "display(df_clean.head())"
      ],
      "metadata": {
        "id": "yPmODF9IRzxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Variable 2: Exports of Fish\n",
        "\n",
        "df_fish_clean = None\n",
        "\n",
        "# Check if data exists in the database\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='fish_exports'\")\n",
        "if cursor.fetchone():\n",
        "    print(\"Loading Fish Exports from SQLite database\")\n",
        "    df_fish_clean = pd.read_sql_query(\"SELECT * FROM fish_exports\", conn)\n",
        "    print(\"Loaded DataFrame from SQLite:\")\n",
        "    # Validate the loaded data\n",
        "    expected_columns = [\"Year\", \"Quarter\", \"Fish_Export_Value_Million_Kr\"]\n",
        "    dtypes = {\"Year\": int, \"Quarter\": str, \"Fish_Export_Value_Million_Kr\": int}\n",
        "    try:\n",
        "        validate_dataframe(df_fish_clean, expected_columns, dtypes)\n",
        "        print(\"Loaded data is valid.\")\n",
        "    except ValueError as e:\n",
        "        print(f\"Validation error: {e}. Dropping and recreating table...\")\n",
        "        cursor.execute(\"DROP TABLE fish_exports\")\n",
        "        conn.commit()\n",
        "        df_fish_clean = None\n",
        "\n",
        "if df_fish_clean is None:\n",
        "    print(\"fish_exports table not found or invalid, querying API...\")\n",
        "    url = \"https://bank.stat.gl:443/api/v1/en/Greenland/BE/BE80/BEXSTA22.px\"\n",
        "    query = {\n",
        "        \"query\": [\n",
        "            {\"code\": \"unit\", \"selection\": {\"filter\": \"item\", \"values\": [\"Mill. kr.\"]}},\n",
        "            {\"code\": \"time\", \"selection\": {\"filter\": \"item\", \"values\": [str(y) for y in range(2011, 2025)]}},\n",
        "            {\"code\": \"quarter\", \"selection\": {\"filter\": \"item\", \"values\": [\"1\", \"2\", \"3\", \"4\"]}}\n",
        "        ],\n",
        "        \"response\": {\"format\": \"json-stat2\"}\n",
        "    }\n",
        "    try:\n",
        "        response = requests.post(url, json=query, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        dataset = pyjstat.Dataset.read(response.text)\n",
        "        df = dataset.write('dataframe')\n",
        "        print(\"Data successfully retrieved and converted to DataFrame!\")\n",
        "\n",
        "        # Clean DataFrame\n",
        "        df_fish_clean = df.copy()\n",
        "        df_fish_clean.rename(columns={\n",
        "            \"time\": \"Year\",\n",
        "            \"quarter\": \"Quarter\",\n",
        "            \"value\": \"Fish_Export_Value_Million_Kr\"\n",
        "        }, inplace=True)\n",
        "        df_fish_clean[\"Quarter\"] = df_fish_clean[\"Quarter\"].str.replace(\"Quarter \", \"Q\")\n",
        "        quarter_order = [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]\n",
        "        df_fish_clean[\"Quarter\"] = pd.Categorical(df_fish_clean[\"Quarter\"], categories=quarter_order, ordered=True)\n",
        "        df_fish_clean = df_fish_clean[[\"Year\", \"Quarter\", \"Fish_Export_Value_Million_Kr\"]]\n",
        "        df_fish_clean[\"Year\"] = df_fish_clean[\"Year\"].astype(int)\n",
        "\n",
        "        # Validate before saving to SQLite\n",
        "        expected_columns = [\"Year\", \"Quarter\", \"Fish_Export_Value_Million_Kr\"]\n",
        "        dtypes = {\"Year\": int, \"Quarter\": str, \"Fish_Export_Value_Million_Kr\": int}\n",
        "        validate_dataframe(df_fish_clean, expected_columns, dtypes)\n",
        "\n",
        "        # Insert data into table (schema already created)\n",
        "        df_fish_clean.to_sql('fish_exports', conn, if_exists='append', index=False)\n",
        "        conn.commit()\n",
        "        print(\"Saved Fish Exports to SQLite table 'fish_exports'\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching Fish Exports data: {e}\")\n",
        "        df_fish_clean = pd.DataFrame(columns=[\"Year\", \"Quarter\", \"Fish_Export_Value_Million_Kr\"])  # Empty DataFrame as fallback\n",
        "\n",
        "# Final display\n",
        "print(\"Final Fish Exports DataFrame:\")\n",
        "display(df_fish_clean.head())"
      ],
      "metadata": {
        "id": "V_sLw7SyR2B7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VARIABLE 3: WEST GREENLAND SST\n",
        "\n",
        "df_sst_west_clean = None\n",
        "\n",
        "# Degree to ERDDAP grid index conversion\n",
        "def deg_to_index_lat(lat): return int(round((lat + 90) / 0.25))\n",
        "def deg_to_index_lon(lon): return int(round((lon + 180) / 0.25))\n",
        "\n",
        "# Define bounding box in degrees (consistent with your prior specifications)\n",
        "bbox_deg = {\n",
        "    'lat_min': 65.0,\n",
        "    'lat_max': 70.0,\n",
        "    'lon_min': -55.0,\n",
        "    'lon_max': -50.0\n",
        "}\n",
        "\n",
        "# Convert to grid indices\n",
        "bbox_idx = {\n",
        "    'lat_min': deg_to_index_lat(bbox_deg['lat_min']),\n",
        "    'lat_max': deg_to_index_lat(bbox_deg['lat_max']),\n",
        "    'lon_min': deg_to_index_lon(bbox_deg['lon_min']),\n",
        "    'lon_max': deg_to_index_lon(bbox_deg['lon_max'])\n",
        "}\n",
        "print(\"Bounding box indices:\", bbox_idx)\n",
        "\n",
        "# Check if data exists in the database\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='sst_west'\")\n",
        "if cursor.fetchone():\n",
        "    print(\"Loading SST West from SQLite database\")\n",
        "    df_sst_west_clean = pd.read_sql_query(\"SELECT * FROM sst_west\", conn)\n",
        "    print(\"Loaded DataFrame from SQLite:\")\n",
        "    expected_columns = [\"Year\", \"Quarter\", \"Sea_Surface_Temp_C_West\", \"Melt_Active_West\", \"Melt_Index_West\"]\n",
        "    dtypes = {\"Year\": int, \"Quarter\": str, \"Sea_Surface_Temp_C_West\": float, \"Melt_Active_West\": int, \"Melt_Index_West\": float}\n",
        "    try:\n",
        "        validate_dataframe(df_sst_west_clean, expected_columns, dtypes)\n",
        "        print(\"Loaded data is valid.\")\n",
        "    except ValueError as e:\n",
        "        print(f\"Validation error: {e}. Dropping and recreating table...\")\n",
        "        cursor.execute(\"DROP TABLE sst_west\")\n",
        "        conn.commit()\n",
        "        create_database_schema()  # Recreate the schema\n",
        "        df_sst_west_clean = None\n",
        "\n",
        "if df_sst_west_clean is None:\n",
        "    print(\"sst_west table not found or invalid, querying API...\")\n",
        "    years = list(range(2011, 2025))\n",
        "    west_quarters = []\n",
        "    for year in years:\n",
        "        print(f\"Processing year: {year}\")\n",
        "        try:\n",
        "            base = \"https://coastwatch.pfeg.noaa.gov/erddap/griddap/ncdcOisst21Agg_LonPM180.csv?\"\n",
        "            var = \"sst\"\n",
        "            time = f\"[({year}-01-01T00:00:00Z):1:({year}-12-31T00:00:00Z)]\".replace(\" \", \"\")\n",
        "            zlev = \"[0:1:0]\"\n",
        "            lat = f\"[({bbox_idx['lat_min']}):1:({bbox_idx['lat_max']})]\"\n",
        "            lon = f\"[({bbox_idx['lon_min']}):1:({bbox_idx['lon_max']})]\"\n",
        "            query = f\"{var}{time}{zlev}{lat}{lon}\"\n",
        "            full_url = base + quote(query, safe=\":/[](),-T\")  # Ensure 'T' is included, as per your past issue\n",
        "            print(\"Constructed URL:\", full_url)\n",
        "\n",
        "            response = requests.get(full_url, timeout=30)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            df = pd.read_csv(StringIO(response.text), skiprows=[1])\n",
        "            df = df.rename(columns={col: col.strip() for col in df.columns})\n",
        "            df = df.dropna(subset=[\"sst\"])\n",
        "\n",
        "            df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
        "            df[\"Year\"] = df[\"time\"].dt.year.astype(int)\n",
        "            df[\"Quarter\"] = \"Q\" + df[\"time\"].dt.quarter.astype(str)\n",
        "\n",
        "            df_q = df.groupby(['Year', 'Quarter'])[\"sst\"].mean().reset_index()\n",
        "            df_q = df_q.rename(columns={\"sst\": \"Sea_Surface_Temp_C_West\"})\n",
        "\n",
        "            df_q[\"Melt_Active_West\"] = (df_q[\"Sea_Surface_Temp_C_West\"] > 0.5).astype(int)\n",
        "            df_q[\"Melt_Index_West\"] = df_q[\"Sea_Surface_Temp_C_West\"].clip(lower=0, upper=4) / 4\n",
        "\n",
        "            west_quarters.append(df_q)\n",
        "            print(f\"{year} processed.\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Failed for {year}: {e}\")\n",
        "            continue  # Continue to the next year instead of failing completely\n",
        "\n",
        "    if west_quarters:\n",
        "        df_sst_west_clean = pd.concat(west_quarters).reset_index(drop=True)\n",
        "        expected_columns = [\"Year\", \"Quarter\", \"Sea_Surface_Temp_C_West\", \"Melt_Active_West\", \"Melt_Index_West\"]\n",
        "        dtypes = {\"Year\": int, \"Quarter\": str, \"Sea_Surface_Temp_C_West\": float, \"Melt_Active_West\": int, \"Melt_Index_West\": float}\n",
        "        validate_dataframe(df_sst_west_clean, expected_columns, dtypes)\n",
        "\n",
        "        df_sst_west_clean.to_sql('sst_west', conn, if_exists='append', index=False)\n",
        "        conn.commit()\n",
        "        print(\"Saved SST West to SQLite table 'sst_west'\")\n",
        "    else:\n",
        "        print(\"No data retrieved for SST West.\")\n",
        "        df_sst_west_clean = pd.DataFrame(columns=[\"Year\", \"Quarter\", \"Sea_Surface_Temp_C_West\", \"Melt_Active_West\", \"Melt_Index_West\"])\n",
        "\n",
        "# Final display\n",
        "print(\"Final SST West DataFrame:\")\n",
        "if df_sst_west_clean is not None:\n",
        "    print(\"Final SST dataset shape:\", df_sst_west_clean.shape)\n",
        "    display(df_sst_west_clean.head())\n",
        "else:\n",
        "    print(\"Error: df_sst_west_clean not created due to API failure.\")"
      ],
      "metadata": {
        "id": "TOcaW3MuR2fG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable 4: East Greenland SST, Melt\n",
        "\n",
        "# VARIABLE 4: EAST GREENLAND SST\n",
        "\n",
        "df_sst_east_clean = None\n",
        "\n",
        "# Degree to ERDDAP grid index conversion\n",
        "def deg_to_index_lat(lat): return int(round((lat + 90) / 0.25))\n",
        "def deg_to_index_lon(lon): return int(round((lon + 180) / 0.25))\n",
        "\n",
        "# Define bounding box in degrees for East Greenland\n",
        "bbox_deg_east = {\n",
        "    'lat_min': 65.0,\n",
        "    'lat_max': 70.0,\n",
        "    'lon_min': -40.0,\n",
        "    'lon_max': -35.0\n",
        "}\n",
        "\n",
        "# Convert to grid indices\n",
        "bbox_idx_east = {\n",
        "    'lat_min': deg_to_index_lat(bbox_deg_east['lat_min']),\n",
        "    'lat_max': deg_to_index_lat(bbox_deg_east['lat_max']),\n",
        "    'lon_min': deg_to_index_lon(bbox_deg_east['lon_min']),\n",
        "    'lon_max': deg_to_index_lon(bbox_deg_east['lon_max'])\n",
        "}\n",
        "print(\"East Greenland bounding box indices:\", bbox_idx_east)\n",
        "\n",
        "# Check if data exists in the database\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='sst_east'\")\n",
        "if cursor.fetchone():\n",
        "    print(\"Loading SST East from SQLite database\")\n",
        "    df_sst_east_clean = pd.read_sql_query(\"SELECT * FROM sst_east\", conn)\n",
        "    print(\"Loaded DataFrame from SQLite:\")\n",
        "    expected_columns = [\"Year\", \"Quarter\", \"Sea_Surface_Temp_C_East\", \"Melt_Active_East\", \"Melt_Index_East\"]\n",
        "    dtypes = {\"Year\": int, \"Quarter\": str, \"Sea_Surface_Temp_C_East\": float, \"Melt_Active_East\": int, \"Melt_Index_East\": float}\n",
        "    try:\n",
        "        validate_dataframe(df_sst_east_clean, expected_columns, dtypes)\n",
        "        print(\"Loaded data is valid.\")\n",
        "    except ValueError as e:\n",
        "        print(f\"Validation error: {e}. Dropping and recreating table...\")\n",
        "        cursor.execute(\"DROP TABLE sst_east\")\n",
        "        conn.commit()\n",
        "        execute_sql_script('ddl_schema.sql')  # Recreate the schema\n",
        "        df_sst_east_clean = None\n",
        "\n",
        "if df_sst_east_clean is None:\n",
        "    print(\"sst_east table not found or invalid, querying API...\")\n",
        "    years = list(range(2011, 2025))\n",
        "    east_quarters = []\n",
        "    for year in years:\n",
        "        print(f\"Processing year: {year}\")\n",
        "        try:\n",
        "            base = \"https://coastwatch.pfeg.noaa.gov/erddap/griddap/ncdcOisst21Agg_LonPM180.csv?\"\n",
        "            var = \"sst\"\n",
        "            time = f\"[({year}-01-01T00:00:00Z):1:({year}-12-31T00:00:00Z)]\".replace(\" \", \"\")\n",
        "            zlev = \"[0:1:0]\"\n",
        "            lat = f\"[({bbox_idx_east['lat_min']}):1:({bbox_idx_east['lat_max']})]\"\n",
        "            lon = f\"[({bbox_idx_east['lon_min']}):1:({bbox_idx_east['lon_max']})]\"\n",
        "            query = f\"{var}{time}{zlev}{lat}{lon}\"\n",
        "            full_url = base + quote(query, safe=\":/[](),-T\")\n",
        "            print(\"Constructed URL:\", full_url)\n",
        "\n",
        "            response = requests.get(full_url, timeout=30)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            df = pd.read_csv(StringIO(response.text), skiprows=[1])\n",
        "            df = df.rename(columns={col: col.strip() for col in df.columns})\n",
        "            df = df.dropna(subset=[\"sst\"])\n",
        "\n",
        "            df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
        "            df[\"Year\"] = df[\"time\"].dt.year.astype(int)\n",
        "            df[\"Quarter\"] = \"Q\" + df[\"time\"].dt.quarter.astype(str)\n",
        "\n",
        "            df_q = df.groupby(['Year', 'Quarter'])[\"sst\"].mean().reset_index()\n",
        "            df_q = df_q.rename(columns={\"sst\": \"Sea_Surface_Temp_C_East\"})\n",
        "\n",
        "            df_q[\"Melt_Active_East\"] = (df_q[\"Sea_Surface_Temp_C_East\"] > 0.5).astype(int)\n",
        "            df_q[\"Melt_Index_East\"] = df_q[\"Sea_Surface_Temp_C_East\"].clip(lower=0, upper=4) / 4\n",
        "\n",
        "            east_quarters.append(df_q)\n",
        "            print(f\"{year} processed.\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Failed for {year}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if east_quarters:\n",
        "        df_sst_east_clean = pd.concat(east_quarters).reset_index(drop=True)\n",
        "        expected_columns = [\"Year\", \"Quarter\", \"Sea_Surface_Temp_C_East\", \"Melt_Active_East\", \"Melt_Index_East\"]\n",
        "        dtypes = {\"Year\": int, \"Quarter\": str, \"Sea_Surface_Temp_C_East\": float, \"Melt_Active_East\": int, \"Melt_Index_East\": float}\n",
        "        validate_dataframe(df_sst_east_clean, expected_columns, dtypes)\n",
        "\n",
        "        df_sst_east_clean.to_sql('sst_east', conn, if_exists='append', index=False)\n",
        "        conn.commit()\n",
        "        print(\"Saved SST East to SQLite table 'sst_east'\")\n",
        "    else:\n",
        "        print(\"No data retrieved for SST East.\")\n",
        "        df_sst_east_clean = pd.DataFrame(columns=[\"Year\", \"Quarter\", \"Sea_Surface_Temp_C_East\", \"Melt_Active_East\", \"Melt_Index_East\"])\n",
        "\n",
        "# Final display\n",
        "print(\"Final SST East DataFrame:\")\n",
        "if df_sst_east_clean is not None:\n",
        "    print(\"Final SST East dataset shape:\", df_sst_east_clean.shape)\n",
        "    display(df_sst_east_clean.head())\n",
        "else:\n",
        "    print(\"Error: df_sst_east_clean not created due to API failure.\")"
      ],
      "metadata": {
        "id": "1nDw7roWSZlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable 5: South Greenland SST, Melt\n",
        "\n",
        "# VARIABLE 5: SOUTH GREENLAND SST\n",
        "\n",
        "df_sst_south_clean = None\n",
        "\n",
        "# Degree to ERDDAP grid index conversion\n",
        "def deg_to_index_lat(lat): return int(round((lat + 90) / 0.25))\n",
        "def deg_to_index_lon(lon): return int(round((lon + 180) / 0.25))\n",
        "\n",
        "# Define bounding box in degrees for South Greenland\n",
        "bbox_deg_south = {\n",
        "    'lat_min': 60.0,\n",
        "    'lat_max': 65.0,\n",
        "    'lon_min': -45.0,\n",
        "    'lon_max': -40.0\n",
        "}\n",
        "\n",
        "# Convert to grid indices\n",
        "bbox_idx_south = {\n",
        "    'lat_min': deg_to_index_lat(bbox_deg_south['lat_min']),\n",
        "    'lat_max': deg_to_index_lat(bbox_deg_south['lat_max']),\n",
        "    'lon_min': deg_to_index_lon(bbox_deg_south['lon_min']),\n",
        "    'lon_max': deg_to_index_lon(bbox_deg_south['lon_max'])\n",
        "}\n",
        "print(\"South Greenland bounding box indices:\", bbox_idx_south)\n",
        "\n",
        "# Check if data exists in the database\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='sst_south'\")\n",
        "if cursor.fetchone():\n",
        "    print(\"Loading SST South from SQLite database\")\n",
        "    df_sst_south_clean = pd.read_sql_query(\"SELECT * FROM sst_south\", conn)\n",
        "    print(\"Loaded DataFrame from SQLite:\")\n",
        "    expected_columns = [\"Year\", \"Quarter\", \"Sea_Surface_Temp_C_South\", \"Melt_Active_South\", \"Melt_Index_South\"]\n",
        "    dtypes = {\"Year\": int, \"Quarter\": str, \"Sea_Surface_Temp_C_South\": float, \"Melt_Active_South\": int, \"Melt_Index_South\": float}\n",
        "    try:\n",
        "        validate_dataframe(df_sst_south_clean, expected_columns, dtypes)\n",
        "        print(\"Loaded data is valid.\")\n",
        "    except ValueError as e:\n",
        "        print(f\"Validation error: {e}. Dropping and recreating table...\")\n",
        "        cursor.execute(\"DROP TABLE sst_south\")\n",
        "        conn.commit()\n",
        "        execute_sql_script('ddl_schema.sql')  # Recreate the schema\n",
        "        df_sst_south_clean = None\n",
        "\n",
        "if df_sst_south_clean is None:\n",
        "    print(\"sst_south table not found or invalid, querying API...\")\n",
        "    years = list(range(2011, 2025))\n",
        "    south_quarters = []\n",
        "    for year in years:\n",
        "        print(f\"Processing year: {year}\")\n",
        "        try:\n",
        "            base = \"https://coastwatch.pfeg.noaa.gov/erddap/griddap/ncdcOisst21Agg_LonPM180.csv?\"\n",
        "            var = \"sst\"\n",
        "            time = f\"[({year}-01-01T00:00:00Z):1:({year}-12-31T00:00:00Z)]\".replace(\" \", \"\")\n",
        "            zlev = \"[0:1:0]\"\n",
        "            lat = f\"[({bbox_idx_south['lat_min']}):1:({bbox_idx_south['lat_max']})]\"\n",
        "            lon = f\"[({bbox_idx_south['lon_min']}):1:({bbox_idx_south['lon_max']})]\"\n",
        "            query = f\"{var}{time}{zlev}{lat}{lon}\"\n",
        "            full_url = base + quote(query, safe=\":/[](),-T\")\n",
        "            print(\"Constructed URL:\", full_url)\n",
        "\n",
        "            response = requests.get(full_url, timeout=30)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            df = pd.read_csv(StringIO(response.text), skiprows=[1])\n",
        "            df = df.rename(columns={col: col.strip() for col in df.columns})\n",
        "            df = df.dropna(subset=[\"sst\"])\n",
        "\n",
        "            df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
        "            df[\"Year\"] = df[\"time\"].dt.year.astype(int)\n",
        "            df[\"Quarter\"] = \"Q\" + df[\"time\"].dt.quarter.astype(str)\n",
        "\n",
        "            df_q = df.groupby(['Year', 'Quarter'])[\"sst\"].mean().reset_index()\n",
        "            df_q = df_q.rename(columns={\"sst\": \"Sea_Surface_Temp_C_South\"})\n",
        "\n",
        "            df_q[\"Melt_Active_South\"] = (df_q[\"Sea_Surface_Temp_C_South\"] > 0.5).astype(int)\n",
        "            df_q[\"Melt_Index_South\"] = df_q[\"Sea_Surface_Temp_C_South\"].clip(lower=0, upper=4) / 4\n",
        "\n",
        "            south_quarters.append(df_q)\n",
        "            print(f\"{year} processed.\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Failed for {year}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if south_quarters:\n",
        "        df_sst_south_clean = pd.concat(south_quarters).reset_index(drop=True)\n",
        "        expected_columns = [\"Year\", \"Quarter\", \"Sea_Surface_Temp_C_South\", \"Melt_Active_South\", \"Melt_Index_South\"]\n",
        "        dtypes = {\"Year\": int, \"Quarter\": str, \"Sea_Surface_Temp_C_South\": float, \"Melt_Active_South\": int, \"Melt_Index_South\": float}\n",
        "        validate_dataframe(df_sst_south_clean, expected_columns, dtypes)\n",
        "\n",
        "        df_sst_south_clean.to_sql('sst_south', conn, if_exists='append', index=False)\n",
        "        conn.commit()\n",
        "        print(\"Saved SST South to SQLite table 'sst_south'\")\n",
        "    else:\n",
        "        print(\"No data retrieved for SST South.\")\n",
        "        df_sst_south_clean = pd.DataFrame(columns=[\"Year\", \"Quarter\", \"Sea_Surface_Temp_C_South\", \"Melt_Active_South\", \"Melt_Index_South\"])\n",
        "\n",
        "# Final display\n",
        "print(\"Final SST South DataFrame:\")\n",
        "if df_sst_south_clean is not None:\n",
        "    print(\"Final SST South dataset shape:\", df_sst_south_clean.shape)\n",
        "    display(df_sst_south_clean.head())\n",
        "else:\n",
        "    print(\"Error: df_sst_south_clean not created due to API failure.\")"
      ],
      "metadata": {
        "id": "nMfok7CfTENF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Variable 6: Total Catch by Foreign Vessels\n",
        "\n",
        "# VARIABLE 6: FOREIGN CATCH\n",
        "\n",
        "df_foreign_clean = None\n",
        "\n",
        "# Check if data exists in the database\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='foreign_catch'\")\n",
        "if cursor.fetchone():\n",
        "    print(\"Loading Foreign Catch from SQLite database\")\n",
        "    df_foreign_clean = pd.read_sql_query(\"SELECT * FROM foreign_catch\", conn)\n",
        "    print(\"Loaded DataFrame from SQLite:\")\n",
        "    expected_columns = [\"Year\", \"Quarter\", \"Unit\", \"Foreign_Catch\"]\n",
        "    dtypes = {\"Year\": int, \"Quarter\": str, \"Unit\": str, \"Foreign_Catch\": int}\n",
        "    try:\n",
        "        validate_dataframe(df_foreign_clean, expected_columns, dtypes)\n",
        "        print(\"Loaded data is valid.\")\n",
        "    except ValueError as e:\n",
        "        print(f\"Validation error: {e}. Dropping and recreating table...\")\n",
        "        cursor.execute(\"DROP TABLE foreign_catch\")\n",
        "        conn.commit()\n",
        "        execute_sql_script('ddl_schema.sql')  # Recreate the schema\n",
        "        df_foreign_clean = None\n",
        "\n",
        "if df_foreign_clean is None:\n",
        "    print(\"foreign_catch table not found or invalid, querying API...\")\n",
        "    url = \"https://bank.stat.gl:443/api/v1/en/Greenland/FI/FI10/FIX008.px\"\n",
        "    query = {\n",
        "        \"query\": [\n",
        "            {\"code\": \"nation\", \"selection\": {\"filter\": \"item\", \"values\": [\"Foreign\"]}},\n",
        "            {\"code\": \"unit\", \"selection\": {\"filter\": \"item\", \"values\": [\"Ton\"]}},\n",
        "            {\"code\": \"time\", \"selection\": {\"filter\": \"item\", \"values\": [str(y) for y in range(2011, 2025)]}},\n",
        "            {\"code\": \"quarter\", \"selection\": {\"filter\": \"item\", \"values\": [\"1\", \"2\", \"3\", \"4\"]}}\n",
        "        ],\n",
        "        \"response\": {\"format\": \"json-stat2\"}\n",
        "    }\n",
        "    try:\n",
        "        response = requests.post(url, json=query, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        dataset = pyjstat.Dataset.read(response.text)\n",
        "        df = dataset.write('dataframe')\n",
        "        print(\"Data successfully retrieved and converted to DataFrame!\")\n",
        "\n",
        "        # Clean DataFrame\n",
        "        df_foreign_clean = df.copy()\n",
        "        df_foreign_clean.drop(columns=['nation'], inplace=True)\n",
        "        df_foreign_clean.rename(columns={\n",
        "            \"time\": \"Year\",\n",
        "            \"quarter\": \"Quarter\",\n",
        "            \"unit\": \"Unit\",\n",
        "            \"value\": \"Foreign_Catch\"\n",
        "        }, inplace=True)\n",
        "        df_foreign_clean[\"Quarter\"] = df_foreign_clean[\"Quarter\"].str.replace(\"Quarter \", \"Q\")\n",
        "        quarter_order = [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]\n",
        "        df_foreign_clean[\"Quarter\"] = pd.Categorical(df_foreign_clean[\"Quarter\"], categories=quarter_order, ordered=True)\n",
        "        df_foreign_clean = df_foreign_clean[[\"Year\", \"Quarter\", \"Unit\", \"Foreign_Catch\"]]\n",
        "        df_foreign_clean[\"Year\"] = df_foreign_clean[\"Year\"].astype(int)\n",
        "\n",
        "        # Validate before saving to SQLite\n",
        "        expected_columns = [\"Year\", \"Quarter\", \"Unit\", \"Foreign_Catch\"]\n",
        "        dtypes = {\"Year\": int, \"Quarter\": str, \"Unit\": str, \"Foreign_Catch\": int}\n",
        "        validate_dataframe(df_foreign_clean, expected_columns, dtypes)\n",
        "\n",
        "        # Insert data into table (schema already created)\n",
        "        df_foreign_clean.to_sql('foreign_catch', conn, if_exists='append', index=False)\n",
        "        conn.commit()\n",
        "        print(\"Saved Foreign Catch to SQLite table 'foreign_catch'\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching Foreign Catch data: {e}\")\n",
        "        df_foreign_clean = pd.DataFrame(columns=[\"Year\", \"Quarter\", \"Unit\", \"Foreign_Catch\"])  # Empty DataFrame as fallback\n",
        "\n",
        "# Final display\n",
        "print(\"Final Foreign Catch DataFrame:\")\n",
        "display(df_foreign_clean.head())"
      ],
      "metadata": {
        "id": "N4TVxXJUSjZv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}