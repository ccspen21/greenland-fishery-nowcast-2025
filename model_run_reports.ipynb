{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccspen21/greenland-fishery-nowcast-2025/blob/main/model_run_reports.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ccspen21/greenland-fishery-nowcast-2025.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYtm_usZKrT1",
        "outputId": "4a49aa8d-5bc8-45de-b97d-2faa4f0eaf9a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'greenland-fishery-nowcast-2025'...\n",
            "remote: Enumerating objects: 55, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (52/52), done.\u001b[K\n",
            "remote: Total 55 (delta 25), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (55/55), 160.09 KiB | 4.21 MiB/s, done.\n",
            "Resolving deltas: 100% (25/25), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests xarray pandas pyjstat datetime pydap netCDF4\n",
        "import os\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time as time_module  # Rename to avoid shadowing\n",
        "from pyjstat import pyjstat\n",
        "from urllib.parse import quote\n",
        "from io import StringIO\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "# Ensure compatibility with Colab and GitHub\n",
        "!apt-get update && apt-get install -y iputils-ping\n",
        "\n",
        "# Define a configurable database path\n",
        "DB_PATH = os.getenv(\"DB_PATH\", \"greenland_fishery.db\")\n",
        "\n",
        "# Helper function to validate DataFrame against schema\n",
        "def validate_dataframe(df, expected_columns, dtypes):\n",
        "    if df.empty:\n",
        "        raise ValueError(\"DataFrame is empty, no rows found.\")\n",
        "    if not all(col in df.columns for col in expected_columns):\n",
        "        raise ValueError(f\"DataFrame missing expected columns: {expected_columns}\")\n",
        "    for col, dtype in dtypes.items():\n",
        "        if col in df.columns:\n",
        "            if dtype == int:\n",
        "                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)\n",
        "            else:\n",
        "                df[col] = df[col].astype(dtype)\n",
        "    if df.isnull().any().any():\n",
        "        raise ValueError(f\"DataFrame contains NaN values: {df.head()}\")\n",
        "\n",
        "# Helper function for API calls with retries and exponential backoff\n",
        "def fetch_with_retries(url, max_retries=3, timeout=60, method='get', json=None):\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            if method == 'get':\n",
        "                response = requests.get(url, timeout=timeout)\n",
        "            else:\n",
        "                response = requests.post(url, json=json, timeout=timeout)\n",
        "            response.raise_for_status()\n",
        "            return response\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Attempt {attempt+1} failed: {e}\")\n",
        "            if attempt + 1 == max_retries:\n",
        "                raise\n",
        "            time_module.sleep(2 ** attempt)  # Use time_module to avoid shadowing"
      ],
      "metadata": {
        "id": "O1qwmnPY5o-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5363a88a-b2df-4c25-b077-5849ce2f5efa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.11/dist-packages (2025.1.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting pyjstat\n",
            "  Downloading pyjstat-2.4.0.tar.gz (798 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.0/798.0 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting datetime\n",
            "  Downloading DateTime-5.5-py3-none-any.whl.metadata (33 kB)\n",
            "Collecting pydap\n",
            "  Downloading pydap-3.5.5-py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting netCDF4\n",
            "  Downloading netCDF4-1.7.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from xarray) (2.0.2)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from xarray) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Collecting zope.interface (from datetime)\n",
            "  Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-cache (from pydap)\n",
            "  Downloading requests_cache-1.2.1-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pydap) (1.14.1)\n",
            "Collecting Webob (from pydap)\n",
            "  Downloading WebOb-1.8.9-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from pydap) (4.13.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from pydap) (5.3.2)\n",
            "Collecting cftime (from netCDF4)\n",
            "  Downloading cftime-1.6.4.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->pydap) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->pydap) (4.13.2)\n",
            "Requirement already satisfied: attrs>=21.2 in /usr/local/lib/python3.11/dist-packages (from requests-cache->pydap) (25.3.0)\n",
            "Collecting cattrs>=22.2 (from requests-cache->pydap)\n",
            "  Downloading cattrs-24.1.3-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests-cache->pydap) (4.3.7)\n",
            "Collecting url-normalize>=1.4 (from requests-cache->pydap)\n",
            "  Downloading url_normalize-2.2.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from zope.interface->datetime) (75.2.0)\n",
            "Downloading DateTime-5.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydap-3.5.5-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading netCDF4-1.7.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cftime-1.6.4.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_cache-1.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/61.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading WebOb-1.8.9-py2.py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.4/115.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.8/259.8 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cattrs-24.1.3-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading url_normalize-2.2.0-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: pyjstat\n",
            "  Building wheel for pyjstat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyjstat: filename=pyjstat-2.4.0-py3-none-any.whl size=22763 sha256=19d607c9d4a0ee37d24129fee60bec0b4fcc1caaa6c92d1e4c52c56d20f79a2f\n",
            "  Stored in directory: /root/.cache/pip/wheels/d7/b0/ac/350c026feabb6a22b2a5b3fb1f694e724c0f57c2c43ccaf738\n",
            "Successfully built pyjstat\n",
            "Installing collected packages: zope.interface, Webob, url-normalize, cftime, cattrs, requests-cache, netCDF4, datetime, pyjstat, pydap\n",
            "Successfully installed Webob-1.8.9 cattrs-24.1.3 cftime-1.6.4.post1 datetime-5.5 netCDF4-1.7.2 pydap-3.5.5 pyjstat-2.4.0 requests-cache-1.2.1 url-normalize-2.2.0 zope.interface-7.2\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,604 kB]\n",
            "Get:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:8 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [75.2 kB]\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,695 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,140 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,842 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,542 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,788 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,243 kB]\n",
            "Fetched 22.2 MB in 3s (8,065 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  iputils-ping\n",
            "0 upgraded, 1 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 42.9 kB of archives.\n",
            "After this operation, 116 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 iputils-ping amd64 3:20211215-1 [42.9 kB]\n",
            "Fetched 42.9 kB in 0s (112 kB/s)\n",
            "Selecting previously unselected package iputils-ping.\n",
            "(Reading database ... 126332 files and directories currently installed.)\n",
            "Preparing to unpack .../iputils-ping_3%3a20211215-1_amd64.deb ...\n",
            "Unpacking iputils-ping (3:20211215-1) ...\n",
            "Setting up iputils-ping (3:20211215-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/greenland-fishery-nowcast-2025\n",
        "\n",
        "import sqlite3\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the pre-populated database in Google Drive (adjust the path as needed)\n",
        "DB_PATH = '/content/drive/MyDrive/greenland_fishery.db'  # Update this path based on where setup_dataset.ipynb saves the database\n",
        "\n",
        "# Connect to the pre-populated SQLite database\n",
        "conn = sqlite3.connect(DB_PATH)\n",
        "cursor = conn.cursor()\n",
        "print(f\"Connected to SQLite database at {DB_PATH}\")\n",
        "\n",
        "# Load DataFrames from the database\n",
        "df_clean = pd.read_sql_query(\"SELECT * FROM total_catch\", conn)\n",
        "df_foreign_clean = pd.read_sql_query(\"SELECT * FROM foreign_catch\", conn)\n",
        "df_fish_clean = pd.read_sql_query(\"SELECT * FROM fish_export\", conn)\n",
        "df_sst_west_clean = pd.read_sql_query(\"SELECT * FROM sst_west\", conn)\n",
        "df_sst_east_clean = pd.read_sql_query(\"SELECT * FROM sst_east\", conn)\n",
        "df_sst_south_clean = pd.read_sql_query(\"SELECT * FROM sst_south\", conn)\n",
        "\n",
        "# Validate loaded DataFrames\n",
        "expected_columns = {\n",
        "    'total_catch': ['Year', 'Quarter', 'Total_Catch', 'Unit'],\n",
        "    'foreign_catch': ['Year', 'Quarter', 'Foreign_Catch', 'Unit'],\n",
        "    'fish_export': ['Year', 'Quarter', 'Fish_Export_Value_Million_Kr', 'Unit'],\n",
        "    'sst_west': ['Year', 'Quarter', 'Sea_Surface_Temp_C_West', 'Melt_Active_West', 'Melt_Index_West'],\n",
        "    'sst_east': ['Year', 'Quarter', 'Sea_Surface_Temp_C_East', 'Melt_Active_East', 'Melt_Index_East'],\n",
        "    'sst_south': ['Year', 'Quarter', 'Sea_Surface_Temp_C_South', 'Melt_Active_South', 'Melt_Index_South']\n",
        "}\n",
        "\n",
        "dtypes = {\n",
        "    'Year': int,\n",
        "    'Quarter': str,\n",
        "    'Total_Catch': float,\n",
        "    'Foreign_Catch': float,\n",
        "    'Fish_Export_Value_Million_Kr': float,\n",
        "    'Sea_Surface_Temp_C_West': float,\n",
        "    'Melt_Active_West': int,\n",
        "    'Melt_Index_West': float,\n",
        "    'Sea_Surface_Temp_C_East': float,\n",
        "    'Melt_Active_East': int,\n",
        "    'Melt_Index_East': float,\n",
        "    'Sea_Surface_Temp_C_South': float,\n",
        "    'Melt_Active_South': int,\n",
        "    'Melt_Index_South': float,\n",
        "    'Unit': str\n",
        "}\n",
        "\n",
        "# Validate each DataFrame\n",
        "validate_dataframe(df_clean, expected_columns['total_catch'], dtypes)\n",
        "validate_dataframe(df_foreign_clean, expected_columns['foreign_catch'], dtypes)\n",
        "validate_dataframe(df_fish_clean, expected_columns['fish_export'], dtypes)\n",
        "validate_dataframe(df_sst_west_clean, expected_columns['sst_west'], dtypes)\n",
        "validate_dataframe(df_sst_east_clean, expected_columns['sst_east'], dtypes)\n",
        "validate_dataframe(df_sst_south_clean, expected_columns['sst_south'], dtypes)\n",
        "\n",
        "print(\"✅ All DataFrames loaded and validated successfully\")"
      ],
      "metadata": {
        "id": "OFMMXVzyKvjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize Year and Quarter columns across all DataFrames\n",
        "def fix_keys(df):\n",
        "    df[\"Year\"] = df[\"Year\"].astype(int)\n",
        "    df[\"Quarter\"] = df[\"Quarter\"].astype(str).str.replace(\"quarter \", \"Q\").str.replace(\"Quarter \", \"Q\")\n",
        "    return df\n",
        "\n",
        "# Apply to all component DataFrames\n",
        "df_clean = fix_keys(df_clean)\n",
        "df_fish_clean = fix_keys(df_fish_clean)\n",
        "df_foreign_clean = fix_keys(df_foreign_clean)\n",
        "df_sst_west_clean = fix_keys(df_sst_west_clean)\n",
        "df_sst_east_clean = fix_keys(df_sst_east_clean)\n",
        "df_sst_south_clean = fix_keys(df_sst_south_clean)\n",
        "\n",
        "print(\"✅ Standardized Year and Quarter columns across all DataFrames\")"
      ],
      "metadata": {
        "id": "A767E_q0MRCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Time Series of Fish Catch"
      ],
      "metadata": {
        "id": "7H42G2U7cPsE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A02AyJpqskn7"
      },
      "outputs": [],
      "source": [
        "# Time Series of Fish Catch\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "df_clean[\"Time\"] = df_clean[\"Year\"].astype(str) + \" \" + df_clean[\"Quarter\"]\n",
        "df_clean[\"Time_Index\"] = range(len(df_clean))\n",
        "\n",
        "# Plot the time series\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(df_clean[\"Time_Index\"], df_clean[\"Total_Catch\"], marker='o', linestyle='-')\n",
        "\n",
        "# Format x-axis: show only one label per year (use Q1 as the anchor)\n",
        "plt.xticks(\n",
        "    ticks=df_clean[df_clean[\"Quarter\"] == \"Q1\"][\"Time_Index\"],\n",
        "    labels=df_clean[df_clean[\"Quarter\"] == \"Q1\"][\"Year\"],\n",
        "    rotation=45\n",
        ")\n",
        "\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Total Catch (Tonnes)\")\n",
        "plt.title(\"Total Fish Catch Over Time\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('total_fish_catch_over_time.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interaction Term"
      ],
      "metadata": {
        "id": "CtmAGUkMcgCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create three-way interaction terms in each SST regional DataFrame\n",
        "df_sst_west_clean[\"Melt_SST_Interaction_West\"] = (\n",
        "    df_sst_west_clean[\"Melt_Active_West\"] *\n",
        "    df_sst_west_clean[\"Melt_Index_West\"] *\n",
        "    df_sst_west_clean[\"Sea_Surface_Temp_C_West\"]\n",
        ")\n",
        "\n",
        "df_sst_east_clean[\"Melt_SST_Interaction_East\"] = (\n",
        "    df_sst_east_clean[\"Melt_Active_East\"] *\n",
        "    df_sst_east_clean[\"Melt_Index_East\"] *\n",
        "    df_sst_east_clean[\"Sea_Surface_Temp_C_East\"]\n",
        ")\n",
        "\n",
        "df_sst_south_clean[\"Melt_SST_Interaction_South\"] = (\n",
        "    df_sst_south_clean[\"Melt_Active_South\"] *\n",
        "    df_sst_south_clean[\"Melt_Index_South\"] *\n",
        "    df_sst_south_clean[\"Sea_Surface_Temp_C_South\"]\n",
        ")"
      ],
      "metadata": {
        "id": "8thEvL9lWD0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merged Dataset"
      ],
      "metadata": {
        "id": "euh2Hsjicr0k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGV5YQrx6c5o"
      },
      "outputs": [],
      "source": [
        "# Merged Dataset\n",
        "# Start fresh from df_clean\n",
        "df_merged_with_interactions = df_clean.copy()\n",
        "\n",
        "# Merge standard right-hand-side variables\n",
        "df_merged_with_interactions = df_merged_with_interactions.merge(df_fish_clean, on=[\"Year\", \"Quarter\"], how=\"inner\")\n",
        "\n",
        "# Merge SST interaction terms\n",
        "df_merged_with_interactions = df_merged_with_interactions.merge(\n",
        "    df_sst_west_clean[[\"Year\", \"Quarter\", \"Melt_SST_Interaction_West\"]],\n",
        "    on=[\"Year\", \"Quarter\"], how=\"inner\"\n",
        ").merge(\n",
        "    df_sst_east_clean[[\"Year\", \"Quarter\", \"Melt_SST_Interaction_East\"]],\n",
        "    on=[\"Year\", \"Quarter\"], how=\"inner\"\n",
        ").merge(\n",
        "    df_sst_south_clean[[\"Year\", \"Quarter\", \"Melt_SST_Interaction_South\"]],\n",
        "    on=[\"Year\", \"Quarter\"], how=\"inner\"\n",
        ")\n",
        "\n",
        "# Merge foreign catch\n",
        "df_merged_with_interactions = df_merged_with_interactions.merge(\n",
        "    df_foreign_clean.drop(columns=[\"Unit\"]),\n",
        "    on=[\"Year\", \"Quarter\"], how=\"left\"\n",
        ")\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df_merged_with_interactions = df_merged_with_interactions.drop(\n",
        "    columns=[col for col in df_merged_with_interactions.columns if \"Unit\" in col], errors=\"ignore\"\n",
        ")\n",
        "df_merged_with_interactions = df_merged_with_interactions.drop(columns=[\"Time\", \"Time_Index\"], errors=\"ignore\")\n",
        "\n",
        "# Order\n",
        "df_merged_with_interactions[\"Quarter\"] = pd.Categorical(\n",
        "    df_merged_with_interactions[\"Quarter\"], categories=[\"Q1\", \"Q2\", \"Q3\", \"Q4\"], ordered=True\n",
        ")\n",
        "df_merged_with_interactions = df_merged_with_interactions.sort_values(by=[\"Year\", \"Quarter\"]).reset_index(drop=True)\n",
        "\n",
        "print(\"✅ Final merged dataset shape:\", df_merged_with_interactions.shape)\n",
        "display(df_merged_with_interactions.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create df_model_with_interactions with Specific Lagged Variables\n",
        "# Step 1: Start with the merged dataset\n",
        "# (The merging already happened in the \"Merged Dataset\" section)\n",
        "\n",
        "# Step 2: Create only the specific lagged variables needed\n",
        "df_merged_with_interactions[\"Total_Catch_lag4\"] = df_merged_with_interactions[\"Total_Catch\"].shift(4)\n",
        "df_merged_with_interactions[\"Foreign_Catch_lag4\"] = df_merged_with_interactions[\"Foreign_Catch\"].shift(4)\n",
        "df_merged_with_interactions[\"Fish_Export_Value_Million_Kr_lag2\"] = df_merged_with_interactions[\"Fish_Export_Value_Million_Kr\"].shift(2)\n",
        "df_merged_with_interactions[\"Melt_SST_Interaction_West_lag4\"] = df_merged_with_interactions[\"Melt_SST_Interaction_West\"].shift(4)\n",
        "df_merged_with_interactions[\"Melt_SST_Interaction_East_lag1\"] = df_merged_with_interactions[\"Melt_SST_Interaction_East\"].shift(1)\n",
        "\n",
        "# Step 3: Define the modeling dataset with only the required features\n",
        "features = [\n",
        "    \"Total_Catch_lag4\",\n",
        "    \"Foreign_Catch_lag4\",\n",
        "    \"Melt_SST_Interaction_West_lag4\",\n",
        "    \"Melt_SST_Interaction_East_lag1\",\n",
        "    \"Fish_Export_Value_Million_Kr_lag2\"\n",
        "]\n",
        "\n",
        "# Drop rows with NA from lagging and select only the required columns\n",
        "df_model_with_interactions = df_merged_with_interactions[\n",
        "    [\"Year\", \"Quarter\", \"Total_Catch\"] + features\n",
        "].dropna().reset_index(drop=True)\n",
        "\n",
        "# Step 4: Define X and y for modeling\n",
        "y = df_model_with_interactions[\"Total_Catch\"]\n",
        "X = df_model_with_interactions[features]\n",
        "\n",
        "# Final shape check\n",
        "print(\"✅ Clean setup with specific lags — X shape:\", X.shape, \"y shape:\", y.shape)\n",
        "print(\"✅ Features used:\", X.columns.tolist())"
      ],
      "metadata": {
        "id": "JtXAmeFC5-Ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary Statistics"
      ],
      "metadata": {
        "id": "6gM4yUEfc9Nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Identify main (non-lagged) variables\n",
        "main_vars = [\n",
        "    col for col in df_merged_with_interactions.columns\n",
        "    if not col.endswith(\"_lag1\")\n",
        "    and not col.endswith(\"_lag2\")\n",
        "    and not col.endswith(\"_lag3\")\n",
        "    and not col.endswith(\"_lag4\")\n",
        "    and col not in [\"Year\", \"Quarter\"]\n",
        "]\n",
        "\n",
        "# Step 2: Subset numeric columns only (optional)\n",
        "main_df = df_merged_with_interactions[main_vars].select_dtypes(include=\"number\")\n",
        "\n",
        "# Step 3: Generate descriptive statistics\n",
        "summary_main = main_df.describe().T\n",
        "summary_main[\"missing\"] = main_df.isna().sum()\n",
        "summary_main[\"skew\"] = main_df.skew()\n",
        "summary_main[\"kurtosis\"] = main_df.kurt()\n",
        "\n",
        "# Round for cleaner presentation\n",
        "summary_main = summary_main.round(2)\n",
        "\n",
        "# Step 4: Display\n",
        "print(\"📊 Descriptive Statistics (Main Variables Only):\")\n",
        "display(summary_main)"
      ],
      "metadata": {
        "id": "5944unKlc8o-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correlation Matrix"
      ],
      "metadata": {
        "id": "P6vZ5fB1dG1H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J08MzwD2LHib"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "main_vars = [\n",
        "    col for col in df_merged_with_interactions.columns\n",
        "    if not col.endswith(\"_lag1\") and not col.endswith(\"_lag2\")\n",
        "    and not col.endswith(\"_lag3\") and not col.endswith(\"_lag4\")\n",
        "    and col not in [\"Year\", \"Quarter\"]\n",
        "]\n",
        "\n",
        "main_corr = df_merged_with_interactions[main_vars].corr()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(main_corr, cmap=\"coolwarm\", annot=True, fmt=\".2f\")\n",
        "plt.title(\"Correlation Matrix of Main (Non-Lagged) Variables\")\n",
        "plt.tight_layout()\n",
        "plt.savefig('correlation_matrix.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nowcasting Q1 2025"
      ],
      "metadata": {
        "id": "iSjkikGWblPg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "od2TJRdboIja"
      },
      "outputs": [],
      "source": [
        "# ✅ Use all available data up to Q4 2024\n",
        "X_train_lasso_q1 = df_model_with_interactions[\n",
        "    (df_model_with_interactions[\"Year\"] < 2025)\n",
        "][[\n",
        "    \"Total_Catch_lag4\",\n",
        "    \"Melt_SST_Interaction_West_lag4\",\n",
        "    \"Foreign_Catch_lag4\",\n",
        "    \"Melt_SST_Interaction_East_lag1\",\n",
        "    \"Fish_Export_Value_Million_Kr_lag2\"\n",
        "]]\n",
        "\n",
        "y_train_lasso_q1 = df_model_with_interactions[\n",
        "    (df_model_with_interactions[\"Year\"] < 2025)\n",
        "][\"Total_Catch\"]\n",
        "\n",
        "# Step 2: Q1 2025 input for LASSO\n",
        "X_nowcast_lasso_q1 = pd.DataFrame([{\n",
        "    \"Total_Catch_lag4\": df_model_with_interactions.iloc[-4][\"Total_Catch_lag4\"],\n",
        "    \"Melt_SST_Interaction_West_lag4\": df_model_with_interactions.iloc[-4][\"Melt_SST_Interaction_West_lag4\"],\n",
        "    \"Foreign_Catch_lag4\": df_model_with_interactions.iloc[-4][\"Foreign_Catch_lag4\"],\n",
        "    \"Melt_SST_Interaction_East_lag1\": df_model_with_interactions.iloc[-1][\"Melt_SST_Interaction_East_lag1\"],\n",
        "    \"Fish_Export_Value_Million_Kr_lag2\": df_model_with_interactions.iloc[-4][\"Fish_Export_Value_Million_Kr_lag2\"]\n",
        "}])\n",
        "\n",
        "# Step 3: LASSO pipeline\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "lasso_pipeline_q1 = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    Lasso(alpha=1.0, max_iter=10000)\n",
        ")\n",
        "\n",
        "lasso_pipeline_q1.fit(X_train_lasso_q1, y_train_lasso_q1)\n",
        "\n",
        "# Step 4: Predict\n",
        "y_pred_lasso_q1 = lasso_pipeline_q1.predict(X_nowcast_lasso_q1)[0]\n",
        "print(f\"📈 LASSO Nowcast for Q1 2025: {round(y_pred_lasso_q1):,.0f} tons\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph of Nowcast"
      ],
      "metadata": {
        "id": "OQy0L8MddqZ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgWKIHkIG8jP"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Rebuild time axis if needed\n",
        "df_clean[\"Time\"] = df_clean[\"Year\"].astype(str) + \" \" + df_clean[\"Quarter\"]\n",
        "df_clean[\"Time_Index\"] = range(len(df_clean))\n",
        "\n",
        "# Define the index and value for Q4 2024 (last actual) and Q1 2025 (nowcast)\n",
        "last_actual_index = df_clean[\"Time_Index\"].max()\n",
        "last_actual_value = df_clean[\"Total_Catch\"].iloc[-1]\n",
        "\n",
        "nowcast_index = last_actual_index + 1\n",
        "nowcast_value = y_pred_lasso_q1\n",
        "\n",
        "# Plot historical series\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df_clean[\"Time_Index\"], df_clean[\"Total_Catch\"], marker='o', linestyle='-', label=\"Historical\")\n",
        "\n",
        "# Add red dot for nowcast\n",
        "plt.plot(nowcast_index, nowcast_value, 'ro', label=\"LASSO Nowcast (Q1 2025)\", markersize=8)\n",
        "\n",
        "# Add red connecting line from Q4 2024 to Q1 2025\n",
        "plt.plot([last_actual_index, nowcast_index], [last_actual_value, nowcast_value], color='red', linestyle='-', linewidth=2)\n",
        "\n",
        "# Update x-ticks to include 2025\n",
        "xticks = list(df_clean[df_clean[\"Quarter\"] == \"Q1\"][\"Time_Index\"])\n",
        "xticks_labels = list(df_clean[df_clean[\"Quarter\"] == \"Q1\"][\"Year\"].astype(str))\n",
        "xticks.append(nowcast_index)\n",
        "xticks_labels.append(\"2025\")\n",
        "\n",
        "plt.xticks(ticks=xticks, labels=xticks_labels, rotation=45)\n",
        "\n",
        "plt.text(\n",
        "    nowcast_index + 0.2,          # shift right\n",
        "    nowcast_value + 2000,         # shift upward\n",
        "    f\"{round(nowcast_value):,}\",\n",
        "    color=\"red\",\n",
        "    fontsize=9.5,\n",
        ")\n",
        "\n",
        "plt.axvspan(nowcast_index - 0.1, nowcast_index + 0.1, color='red', alpha=0.1)\n",
        "\n",
        "plt.axvline(nowcast_index, color='gray', linestyle='--', linewidth=1)\n",
        "\n",
        "# Labels and formatting\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Total Catch (Tonnes)\")\n",
        "plt.title(\"Total Fish Catch Over Time with LASSO Nowcast\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('total_fish_catch_with_nowcast.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Capelin Dummy - Example of Policy Intervention"
      ],
      "metadata": {
        "id": "PpJm0uVwd-53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note on Capelin Dummy\n",
        "The Capelin_Open dummy variable is introduced here to model the impact of policy interventions (e.g., fishery closures in 2019, 2020, 2024, and 2025). This variable is used specifically for nowcasting in this notebook and is not included in the LASSO model fitting performed in model_fitting_diagnostics.ipynb. This allows us to isolate the effect of policy shocks in our nowcast analysis.\n"
      ],
      "metadata": {
        "id": "677-Byes6ICz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWNVEKSNIFDn"
      },
      "outputs": [],
      "source": [
        "# Add Capelin_Open dummy variable to the existing df_model_with_interactions\n",
        "df_model_capelin = df_model_with_interactions.copy()\n",
        "\n",
        "# Add Capelin_Open dummy variable\n",
        "df_model_capelin[\"Capelin_Open\"] = df_model_capelin[\"Year\"].apply(\n",
        "    lambda x: 0 if x in [2019, 2020, 2024, 2025] else 1\n",
        ")\n",
        "\n",
        "# Define target and features (include Capelin_Open)\n",
        "y_capelin = df_model_capelin[\"Total_Catch\"]\n",
        "X_capelin = df_model_capelin[\n",
        "    [\n",
        "        \"Total_Catch_lag4\",\n",
        "        \"Foreign_Catch_lag4\",\n",
        "        \"Melt_SST_Interaction_West_lag4\",\n",
        "        \"Melt_SST_Interaction_East_lag1\",\n",
        "        \"Fish_Export_Value_Million_Kr_lag2\",\n",
        "        \"Capelin_Open\"\n",
        "    ]\n",
        "]\n",
        "\n",
        "# Final check\n",
        "print(\"✅ X_capelin shape:\", X_capelin.shape)\n",
        "print(\"✅ y_capelin shape:\", y_capelin.shape)\n",
        "print(\"✅ Final predictors:\", X_capelin.columns.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nowcast with Capelin Dummy"
      ],
      "metadata": {
        "id": "f9p1NRDfeMqM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b48t71xQJrjh"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 1: Training data (up to Q4 2024)\n",
        "X_train_q1_2025_binary = df_model_capelin[\n",
        "    (df_model_capelin[\"Year\"] < 2025)\n",
        "][[\n",
        "    \"Total_Catch_lag4\",\n",
        "    \"Foreign_Catch_lag4\",\n",
        "    \"Melt_SST_Interaction_West_lag4\",\n",
        "    \"Melt_SST_Interaction_East_lag1\",\n",
        "    \"Fish_Export_Value_Million_Kr_lag2\",\n",
        "    \"Capelin_Open\"\n",
        "]]\n",
        "y_train_q1_2025_binary = df_model_capelin[\n",
        "    (df_model_capelin[\"Year\"] < 2025)\n",
        "][\"Total_Catch\"]\n",
        "\n",
        "print(\"✅ Training set size with Capelin dummy:\", X_train_q1_2025_binary.shape)\n",
        "\n",
        "# Step 2: Input row for Q1 2025\n",
        "X_nowcast_q1_2025_binary = pd.DataFrame([{\n",
        "    \"Total_Catch_lag4\": df_model_capelin.iloc[-4][\"Total_Catch_lag4\"],\n",
        "    \"Foreign_Catch_lag4\": df_model_capelin.iloc[-4][\"Foreign_Catch_lag4\"],\n",
        "    \"Melt_SST_Interaction_West_lag4\": df_model_capelin.iloc[-4][\"Melt_SST_Interaction_West_lag4\"],\n",
        "    \"Melt_SST_Interaction_East_lag1\": df_model_capelin.iloc[-1][\"Melt_SST_Interaction_East_lag1\"],\n",
        "    \"Fish_Export_Value_Million_Kr_lag2\": df_model_capelin.iloc[-4][\"Fish_Export_Value_Million_Kr_lag2\"],\n",
        "    \"Capelin_Open\": 0  # manually set to 0 because 2025 is closed\n",
        "}])\n",
        "\n",
        "print(\"✅ Nowcast input for Q1 2025 with Capelin dummy:\")\n",
        "display(X_nowcast_q1_2025_binary)\n",
        "\n",
        "# Step 3: Lasso model pipeline\n",
        "lasso_pipeline_q1_binary = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    Lasso(alpha=1.0, max_iter=50000, random_state=42)\n",
        ")\n",
        "\n",
        "# Step 4: Fit and predict\n",
        "lasso_pipeline_q1_binary.fit(X_train_q1_2025_binary, y_train_q1_2025_binary)\n",
        "y_pred_q1_2025_binary = lasso_pipeline_q1_binary.predict(X_nowcast_q1_2025_binary)[0]\n",
        "\n",
        "# Step 5: Output result\n",
        "print(f\"📈 📊 Lasso Nowcast for Q1 2025 (with Capelin dummy): {round(y_pred_q1_2025_binary):,.0f} tons\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graph Comparison of Nowcasts"
      ],
      "metadata": {
        "id": "mqG16rGLeUHN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrI2nK_nJr6d"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Rebuild time axis if needed\n",
        "df_clean[\"Time\"] = df_clean[\"Year\"].astype(str) + \" \" + df_clean[\"Quarter\"]\n",
        "df_clean[\"Time_Index\"] = range(len(df_clean))\n",
        "\n",
        "# Final observed point (Q4 2024)\n",
        "last_actual_index = df_clean[\"Time_Index\"].max()\n",
        "last_actual_value = df_clean[\"Total_Catch\"].iloc[-1]\n",
        "\n",
        "# Prediction point (Q1 2025)\n",
        "nowcast_index = last_actual_index + 1\n",
        "nowcast_std = y_pred_lasso_q1                 # Standard LASSO (no Capelin)\n",
        "nowcast_binary = y_pred_q1_2025_binary        # LASSO with Capelin dummy\n",
        "\n",
        "# Plot historical data\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df_clean[\"Time_Index\"], df_clean[\"Total_Catch\"], marker='o', linestyle='-', label=\"Historical\")\n",
        "\n",
        "# Plot both nowcasts as dots\n",
        "plt.plot(nowcast_index, nowcast_std, 'ro', label=\"LASSO Nowcast (No Capelin)\", markersize=8)\n",
        "plt.plot(nowcast_index, nowcast_binary, 'go', label=\"LASSO Nowcast (With Capelin)\", markersize=8)\n",
        "\n",
        "# Connect last actual to each forecast\n",
        "plt.plot([last_actual_index, nowcast_index], [last_actual_value, nowcast_std],\n",
        "         color='red', linestyle='-', linewidth=2)\n",
        "plt.plot([last_actual_index, nowcast_index], [last_actual_value, nowcast_binary],\n",
        "         color='green', linestyle='-', linewidth=2)\n",
        "\n",
        "# Annotate both nowcast values\n",
        "plt.text(nowcast_index + 0.2, nowcast_std + 2000,\n",
        "         f\"{round(nowcast_std):,}\", color=\"red\", fontsize=9.5)\n",
        "\n",
        "plt.text(nowcast_index + 0.2, nowcast_binary + 2000,\n",
        "         f\"{round(nowcast_binary):,}\", color=\"green\", fontsize=9.5)\n",
        "\n",
        "# Forecast area shading and vertical marker\n",
        "plt.axvspan(nowcast_index - 0.1, nowcast_index + 0.1, color='gray', alpha=0.1)\n",
        "plt.axvline(nowcast_index, color='gray', linestyle='--', linewidth=1)\n",
        "\n",
        "# X-axis setup\n",
        "xticks = list(df_clean[df_clean[\"Quarter\"] == \"Q1\"][\"Time_Index\"])\n",
        "xticks_labels = list(df_clean[df_clean[\"Quarter\"] == \"Q1\"][\"Year\"].astype(str))\n",
        "xticks.append(nowcast_index)\n",
        "xticks_labels.append(\"2025\")\n",
        "plt.xticks(ticks=xticks, labels=xticks_labels, rotation=45)\n",
        "\n",
        "# Labels and formatting\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Total Catch (Tonnes)\")\n",
        "plt.title(\"Total Fish Catch Over Time: LASSO Nowcast With and Without Capelin Dummy\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('nowcast_comparison.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actual vs. Predicted Plot for Backtests"
      ],
      "metadata": {
        "id": "DNerKdzLdebk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0FBO0cuElWz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the quarters and values\n",
        "quarters = [\"Q1 2024\", \"Q2 2024\", \"Q3 2024\", \"Q4 2024\"]\n",
        "actual_values = [actual_q1, actual_q2, actual_q3, actual_q4]\n",
        "predicted_values = [\n",
        "    y_pred_q1_2024_lasso,\n",
        "    y_pred_q2_2024_lasso,\n",
        "    y_pred_q3_2024_lasso,\n",
        "    y_pred_q4_2024_lasso\n",
        "]\n",
        "\n",
        "# Calculate y-axis padding\n",
        "all_values = actual_values + predicted_values\n",
        "y_min = min(all_values) * 0.9  # 10% below\n",
        "y_max = max(all_values) * 1.1  # 10% above\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(quarters, actual_values, label=\"Actual\", marker='o', linewidth=2)\n",
        "plt.plot(quarters, predicted_values, label=\"LASSO Prediction\", marker='o', linewidth=2)\n",
        "plt.ylim(y_min, y_max)\n",
        "plt.title(\"📊 Actual vs. Predicted Fish Catch (LASSO, Q1–Q4 2024)\")\n",
        "plt.xlabel(\"Quarter\")\n",
        "plt.ylabel(\"Total_Catch (tons)\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('actual_vs_predicted.png')"
      ]
    }
  ]
}