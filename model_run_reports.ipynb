{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccspen21/greenland-fishery-nowcast-2025/blob/main/model_run_reports.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ccspen21/greenland-fishery-nowcast-2025.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYtm_usZKrT1",
        "outputId": "ac8a2e42-5ac2-4ca0-af07-e2aef93114e5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'greenland-fishery-nowcast-2025' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests xarray pandas pyjstat datetime pydap netCDF4\n",
        "import os\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time as time_module  # Rename to avoid shadowing\n",
        "from pyjstat import pyjstat\n",
        "from urllib.parse import quote\n",
        "from io import StringIO\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "# Ensure compatibility with Colab and GitHub\n",
        "!apt-get update && apt-get install -y iputils-ping\n",
        "\n",
        "# Define a configurable database path\n",
        "DB_PATH = os.getenv(\"DB_PATH\", \"greenland_fishery.db\")\n",
        "\n",
        "# Helper function to validate DataFrame against schema\n",
        "def validate_dataframe(df, expected_columns, dtypes):\n",
        "    if df.empty:\n",
        "        raise ValueError(\"DataFrame is empty, no rows found.\")\n",
        "    if not all(col in df.columns for col in expected_columns):\n",
        "        raise ValueError(f\"DataFrame missing expected columns: {expected_columns}\")\n",
        "    for col, dtype in dtypes.items():\n",
        "        if col in df.columns:\n",
        "            if dtype == int:\n",
        "                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)\n",
        "            else:\n",
        "                df[col] = df[col].astype(dtype)\n",
        "    if df.isnull().any().any():\n",
        "        raise ValueError(f\"DataFrame contains NaN values: {df.head()}\")\n",
        "\n",
        "# Helper function for API calls with retries and exponential backoff\n",
        "def fetch_with_retries(url, max_retries=3, timeout=60, method='get', json=None):\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            if method == 'get':\n",
        "                response = requests.get(url, timeout=timeout)\n",
        "            else:\n",
        "                response = requests.post(url, json=json, timeout=timeout)\n",
        "            response.raise_for_status()\n",
        "            return response\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Attempt {attempt+1} failed: {e}\")\n",
        "            if attempt + 1 == max_retries:\n",
        "                raise\n",
        "            time_module.sleep(2 ** attempt)  # Use time_module to avoid shadowing"
      ],
      "metadata": {
        "id": "O1qwmnPY5o-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d00224b-e2bc-4f3d-cb42-936d673d83b6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.11/dist-packages (2025.1.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pyjstat in /usr/local/lib/python3.11/dist-packages (2.4.0)\n",
            "Requirement already satisfied: datetime in /usr/local/lib/python3.11/dist-packages (5.5)\n",
            "Requirement already satisfied: pydap in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: netCDF4 in /usr/local/lib/python3.11/dist-packages (1.7.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from xarray) (2.0.2)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from xarray) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: zope.interface in /usr/local/lib/python3.11/dist-packages (from datetime) (7.2)\n",
            "Requirement already satisfied: requests-cache in /usr/local/lib/python3.11/dist-packages (from pydap) (1.2.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pydap) (1.14.1)\n",
            "Requirement already satisfied: Webob in /usr/local/lib/python3.11/dist-packages (from pydap) (1.8.9)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from pydap) (4.13.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from pydap) (5.3.2)\n",
            "Requirement already satisfied: cftime in /usr/local/lib/python3.11/dist-packages (from netCDF4) (1.6.4.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->pydap) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->pydap) (4.13.2)\n",
            "Requirement already satisfied: attrs>=21.2 in /usr/local/lib/python3.11/dist-packages (from requests-cache->pydap) (25.3.0)\n",
            "Requirement already satisfied: cattrs>=22.2 in /usr/local/lib/python3.11/dist-packages (from requests-cache->pydap) (24.1.3)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests-cache->pydap) (4.3.7)\n",
            "Requirement already satisfied: url-normalize>=1.4 in /usr/local/lib/python3.11/dist-packages (from requests-cache->pydap) (2.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from zope.interface->datetime) (75.2.0)\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 257 kB in 1s (175 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "iputils-ping is already the newest version (3:20211215-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/greenland-fishery-nowcast-2025\n",
        "\n",
        "import sqlite3\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the pre-populated database in Google Drive (adjust the path as needed)\n",
        "DB_PATH = '/content/drive/MyDrive/greenland_fishery.db'  # Update this path based on where setup_dataset.ipynb saves the database\n",
        "\n",
        "# Connect to the pre-populated SQLite database\n",
        "conn = sqlite3.connect(DB_PATH)\n",
        "cursor = conn.cursor()\n",
        "print(f\"Connected to SQLite database at {DB_PATH}\")\n",
        "\n",
        "# Load DataFrames from the database\n",
        "df_clean = pd.read_sql_query(\"SELECT * FROM total_catch\", conn)\n",
        "df_foreign_clean = pd.read_sql_query(\"SELECT * FROM foreign_catch\", conn)\n",
        "df_fish_clean = pd.read_sql_query(\"SELECT * FROM fish_export\", conn)\n",
        "df_sst_west_clean = pd.read_sql_query(\"SELECT * FROM sst_west\", conn)\n",
        "df_sst_east_clean = pd.read_sql_query(\"SELECT * FROM sst_east\", conn)\n",
        "df_sst_south_clean = pd.read_sql_query(\"SELECT * FROM sst_south\", conn)\n",
        "\n",
        "# Validate loaded DataFrames\n",
        "expected_columns = {\n",
        "    'total_catch': ['Year', 'Quarter', 'Total_Catch', 'Unit'],\n",
        "    'foreign_catch': ['Year', 'Quarter', 'Foreign_Catch', 'Unit'],\n",
        "    'fish_export': ['Year', 'Quarter', 'Fish_Export_Value_Million_Kr', 'Unit'],\n",
        "    'sst_west': ['Year', 'Quarter', 'Sea_Surface_Temp_C_West', 'Melt_Active_West', 'Melt_Index_West'],\n",
        "    'sst_east': ['Year', 'Quarter', 'Sea_Surface_Temp_C_East', 'Melt_Active_East', 'Melt_Index_East'],\n",
        "    'sst_south': ['Year', 'Quarter', 'Sea_Surface_Temp_C_South', 'Melt_Active_South', 'Melt_Index_South']\n",
        "}\n",
        "\n",
        "dtypes = {\n",
        "    'Year': int,\n",
        "    'Quarter': str,\n",
        "    'Total_Catch': float,\n",
        "    'Foreign_Catch': float,\n",
        "    'Fish_Export_Value_Million_Kr': float,\n",
        "    'Sea_Surface_Temp_C_West': float,\n",
        "    'Melt_Active_West': int,\n",
        "    'Melt_Index_West': float,\n",
        "    'Sea_Surface_Temp_C_East': float,\n",
        "    'Melt_Active_East': int,\n",
        "    'Melt_Index_East': float,\n",
        "    'Sea_Surface_Temp_C_South': float,\n",
        "    'Melt_Active_South': int,\n",
        "    'Melt_Index_South': float,\n",
        "    'Unit': str\n",
        "}\n",
        "\n",
        "# Validate each DataFrame\n",
        "validate_dataframe(df_clean, expected_columns['total_catch'], dtypes)\n",
        "validate_dataframe(df_foreign_clean, expected_columns['foreign_catch'], dtypes)\n",
        "validate_dataframe(df_fish_clean, expected_columns['fish_export'], dtypes)\n",
        "validate_dataframe(df_sst_west_clean, expected_columns['sst_west'], dtypes)\n",
        "validate_dataframe(df_sst_east_clean, expected_columns['sst_east'], dtypes)\n",
        "validate_dataframe(df_sst_south_clean, expected_columns['sst_south'], dtypes)\n",
        "\n",
        "print(\"✅ All DataFrames loaded and validated successfully\")"
      ],
      "metadata": {
        "id": "OFMMXVzyKvjY",
        "outputId": "df9d4cc8-89fc-4b60-dd4a-a53fd833f53d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/greenland-fishery-nowcast-2025\n",
            "Mounted at /content/drive\n",
            "Connected to SQLite database at /content/drive/MyDrive/greenland_fishery.db\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "DatabaseError",
          "evalue": "Execution failed on sql 'SELECT * FROM fish_export': no such table: fish_export",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, sql, params)\u001b[0m\n\u001b[1;32m   2673\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2674\u001b[0;31m             \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2675\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOperationalError\u001b[0m: no such table: fish_export",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-aacc4415b639>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mdf_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * FROM total_catch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mdf_foreign_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * FROM foreign_catch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdf_fish_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * FROM fish_export\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mdf_sst_west_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * FROM sst_west\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mdf_sst_east_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * FROM sst_east\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mread_sql_query\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mpandasSQL_builder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpandas_sql\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         return pandas_sql.read_query(\n\u001b[0m\u001b[1;32m    527\u001b[0m             \u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mread_query\u001b[0;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[1;32m   2736\u001b[0m         \u001b[0mdtype_backend\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDtypeBackend\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"numpy\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"numpy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2737\u001b[0m     ) -> DataFrame | Iterator[DataFrame]:\n\u001b[0;32m-> 2738\u001b[0;31m         \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2739\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol_desc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol_desc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, sql, params)\u001b[0m\n\u001b[1;32m   2684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2685\u001b[0m             \u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatabaseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Execution failed on sql '{sql}': {exc}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2686\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2688\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDatabaseError\u001b[0m: Execution failed on sql 'SELECT * FROM fish_export': no such table: fish_export"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize Year and Quarter columns across all DataFrames\n",
        "def fix_keys(df):\n",
        "    df[\"Year\"] = df[\"Year\"].astype(int)\n",
        "    df[\"Quarter\"] = df[\"Quarter\"].astype(str).str.replace(\"quarter \", \"Q\").str.replace(\"Quarter \", \"Q\")\n",
        "    return df\n",
        "\n",
        "# Apply to all component DataFrames\n",
        "df_clean = fix_keys(df_clean)\n",
        "df_fish_clean = fix_keys(df_fish_clean)\n",
        "df_foreign_clean = fix_keys(df_foreign_clean)\n",
        "df_sst_west_clean = fix_keys(df_sst_west_clean)\n",
        "df_sst_east_clean = fix_keys(df_sst_east_clean)\n",
        "df_sst_south_clean = fix_keys(df_sst_south_clean)\n",
        "\n",
        "print(\"✅ Standardized Year and Quarter columns across all DataFrames\")"
      ],
      "metadata": {
        "id": "A767E_q0MRCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Time Series of Fish Catch"
      ],
      "metadata": {
        "id": "7H42G2U7cPsE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A02AyJpqskn7"
      },
      "outputs": [],
      "source": [
        "# Time Series of Fish Catch\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "df_clean[\"Time\"] = df_clean[\"Year\"].astype(str) + \" \" + df_clean[\"Quarter\"]\n",
        "df_clean[\"Time_Index\"] = range(len(df_clean))\n",
        "\n",
        "# Plot the time series\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(df_clean[\"Time_Index\"], df_clean[\"Total_Catch\"], marker='o', linestyle='-')\n",
        "\n",
        "# Format x-axis: show only one label per year (use Q1 as the anchor)\n",
        "plt.xticks(\n",
        "    ticks=df_clean[df_clean[\"Quarter\"] == \"Q1\"][\"Time_Index\"],\n",
        "    labels=df_clean[df_clean[\"Quarter\"] == \"Q1\"][\"Year\"],\n",
        "    rotation=45\n",
        ")\n",
        "\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Total Catch (Tonnes)\")\n",
        "plt.title(\"Total Fish Catch Over Time\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('total_fish_catch_over_time.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interaction Term"
      ],
      "metadata": {
        "id": "CtmAGUkMcgCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create three-way interaction terms in each SST regional DataFrame\n",
        "df_sst_west_clean[\"Melt_SST_Interaction_West\"] = (\n",
        "    df_sst_west_clean[\"Melt_Active_West\"] *\n",
        "    df_sst_west_clean[\"Melt_Index_West\"] *\n",
        "    df_sst_west_clean[\"Sea_Surface_Temp_C_West\"]\n",
        ")\n",
        "\n",
        "df_sst_east_clean[\"Melt_SST_Interaction_East\"] = (\n",
        "    df_sst_east_clean[\"Melt_Active_East\"] *\n",
        "    df_sst_east_clean[\"Melt_Index_East\"] *\n",
        "    df_sst_east_clean[\"Sea_Surface_Temp_C_East\"]\n",
        ")\n",
        "\n",
        "df_sst_south_clean[\"Melt_SST_Interaction_South\"] = (\n",
        "    df_sst_south_clean[\"Melt_Active_South\"] *\n",
        "    df_sst_south_clean[\"Melt_Index_South\"] *\n",
        "    df_sst_south_clean[\"Sea_Surface_Temp_C_South\"]\n",
        ")"
      ],
      "metadata": {
        "id": "8thEvL9lWD0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merged Dataset"
      ],
      "metadata": {
        "id": "euh2Hsjicr0k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGV5YQrx6c5o"
      },
      "outputs": [],
      "source": [
        "# Merged Dataset\n",
        "# Start fresh from df_clean\n",
        "df_merged_with_interactions = df_clean.copy()\n",
        "\n",
        "# Merge standard right-hand-side variables\n",
        "df_merged_with_interactions = df_merged_with_interactions.merge(df_fish_clean, on=[\"Year\", \"Quarter\"], how=\"inner\")\n",
        "\n",
        "# Merge SST interaction terms\n",
        "df_merged_with_interactions = df_merged_with_interactions.merge(\n",
        "    df_sst_west_clean[[\"Year\", \"Quarter\", \"Melt_SST_Interaction_West\"]],\n",
        "    on=[\"Year\", \"Quarter\"], how=\"inner\"\n",
        ").merge(\n",
        "    df_sst_east_clean[[\"Year\", \"Quarter\", \"Melt_SST_Interaction_East\"]],\n",
        "    on=[\"Year\", \"Quarter\"], how=\"inner\"\n",
        ").merge(\n",
        "    df_sst_south_clean[[\"Year\", \"Quarter\", \"Melt_SST_Interaction_South\"]],\n",
        "    on=[\"Year\", \"Quarter\"], how=\"inner\"\n",
        ")\n",
        "\n",
        "# Merge foreign catch\n",
        "df_merged_with_interactions = df_merged_with_interactions.merge(\n",
        "    df_foreign_clean.drop(columns=[\"Unit\"]),\n",
        "    on=[\"Year\", \"Quarter\"], how=\"left\"\n",
        ")\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df_merged_with_interactions = df_merged_with_interactions.drop(\n",
        "    columns=[col for col in df_merged_with_interactions.columns if \"Unit\" in col], errors=\"ignore\"\n",
        ")\n",
        "df_merged_with_interactions = df_merged_with_interactions.drop(columns=[\"Time\", \"Time_Index\"], errors=\"ignore\")\n",
        "\n",
        "# Order\n",
        "df_merged_with_interactions[\"Quarter\"] = pd.Categorical(\n",
        "    df_merged_with_interactions[\"Quarter\"], categories=[\"Q1\", \"Q2\", \"Q3\", \"Q4\"], ordered=True\n",
        ")\n",
        "df_merged_with_interactions = df_merged_with_interactions.sort_values(by=[\"Year\", \"Quarter\"]).reset_index(drop=True)\n",
        "\n",
        "print(\"✅ Final merged dataset shape:\", df_merged_with_interactions.shape)\n",
        "display(df_merged_with_interactions.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create df_model_with_interactions with Specific Lagged Variables\n",
        "# Step 1: Start with the merged dataset\n",
        "# (The merging already happened in the \"Merged Dataset\" section)\n",
        "\n",
        "# Step 2: Create only the specific lagged variables needed\n",
        "df_merged_with_interactions[\"Total_Catch_lag4\"] = df_merged_with_interactions[\"Total_Catch\"].shift(4)\n",
        "df_merged_with_interactions[\"Foreign_Catch_lag4\"] = df_merged_with_interactions[\"Foreign_Catch\"].shift(4)\n",
        "df_merged_with_interactions[\"Fish_Export_Value_Million_Kr_lag2\"] = df_merged_with_interactions[\"Fish_Export_Value_Million_Kr\"].shift(2)\n",
        "df_merged_with_interactions[\"Melt_SST_Interaction_West_lag4\"] = df_merged_with_interactions[\"Melt_SST_Interaction_West\"].shift(4)\n",
        "df_merged_with_interactions[\"Melt_SST_Interaction_East_lag1\"] = df_merged_with_interactions[\"Melt_SST_Interaction_East\"].shift(1)\n",
        "\n",
        "# Step 3: Define the modeling dataset with only the required features\n",
        "features = [\n",
        "    \"Total_Catch_lag4\",\n",
        "    \"Foreign_Catch_lag4\",\n",
        "    \"Melt_SST_Interaction_West_lag4\",\n",
        "    \"Melt_SST_Interaction_East_lag1\",\n",
        "    \"Fish_Export_Value_Million_Kr_lag2\"\n",
        "]\n",
        "\n",
        "# Drop rows with NA from lagging and select only the required columns\n",
        "df_model_with_interactions = df_merged_with_interactions[\n",
        "    [\"Year\", \"Quarter\", \"Total_Catch\"] + features\n",
        "].dropna().reset_index(drop=True)\n",
        "\n",
        "# Step 4: Define X and y for modeling\n",
        "y = df_model_with_interactions[\"Total_Catch\"]\n",
        "X = df_model_with_interactions[features]\n",
        "\n",
        "# Final shape check\n",
        "print(\"✅ Clean setup with specific lags — X shape:\", X.shape, \"y shape:\", y.shape)\n",
        "print(\"✅ Features used:\", X.columns.tolist())"
      ],
      "metadata": {
        "id": "JtXAmeFC5-Ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary Statistics"
      ],
      "metadata": {
        "id": "6gM4yUEfc9Nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Identify main (non-lagged) variables\n",
        "main_vars = [\n",
        "    col for col in df_merged_with_interactions.columns\n",
        "    if not col.endswith(\"_lag1\")\n",
        "    and not col.endswith(\"_lag2\")\n",
        "    and not col.endswith(\"_lag3\")\n",
        "    and not col.endswith(\"_lag4\")\n",
        "    and col not in [\"Year\", \"Quarter\"]\n",
        "]\n",
        "\n",
        "# Step 2: Subset numeric columns only (optional)\n",
        "main_df = df_merged_with_interactions[main_vars].select_dtypes(include=\"number\")\n",
        "\n",
        "# Step 3: Generate descriptive statistics\n",
        "summary_main = main_df.describe().T\n",
        "summary_main[\"missing\"] = main_df.isna().sum()\n",
        "summary_main[\"skew\"] = main_df.skew()\n",
        "summary_main[\"kurtosis\"] = main_df.kurt()\n",
        "\n",
        "# Round for cleaner presentation\n",
        "summary_main = summary_main.round(2)\n",
        "\n",
        "# Step 4: Display\n",
        "print(\"📊 Descriptive Statistics (Main Variables Only):\")\n",
        "display(summary_main)"
      ],
      "metadata": {
        "id": "5944unKlc8o-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correlation Matrix"
      ],
      "metadata": {
        "id": "P6vZ5fB1dG1H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J08MzwD2LHib"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "main_vars = [\n",
        "    col for col in df_merged_with_interactions.columns\n",
        "    if not col.endswith(\"_lag1\") and not col.endswith(\"_lag2\")\n",
        "    and not col.endswith(\"_lag3\") and not col.endswith(\"_lag4\")\n",
        "    and col not in [\"Year\", \"Quarter\"]\n",
        "]\n",
        "\n",
        "main_corr = df_merged_with_interactions[main_vars].corr()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(main_corr, cmap=\"coolwarm\", annot=True, fmt=\".2f\")\n",
        "plt.title(\"Correlation Matrix of Main (Non-Lagged) Variables\")\n",
        "plt.tight_layout()\n",
        "plt.savefig('correlation_matrix.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nowcasting Q1 2025"
      ],
      "metadata": {
        "id": "iSjkikGWblPg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "od2TJRdboIja"
      },
      "outputs": [],
      "source": [
        "# ✅ Use all available data up to Q4 2024\n",
        "X_train_lasso_q1 = df_model_with_interactions[\n",
        "    (df_model_with_interactions[\"Year\"] < 2025)\n",
        "][[\n",
        "    \"Total_Catch_lag4\",\n",
        "    \"Melt_SST_Interaction_West_lag4\",\n",
        "    \"Foreign_Catch_lag4\",\n",
        "    \"Melt_SST_Interaction_East_lag1\",\n",
        "    \"Fish_Export_Value_Million_Kr_lag2\"\n",
        "]]\n",
        "\n",
        "y_train_lasso_q1 = df_model_with_interactions[\n",
        "    (df_model_with_interactions[\"Year\"] < 2025)\n",
        "][\"Total_Catch\"]\n",
        "\n",
        "# Step 2: Q1 2025 input for LASSO\n",
        "X_nowcast_lasso_q1 = pd.DataFrame([{\n",
        "    \"Total_Catch_lag4\": df_model_with_interactions.iloc[-4][\"Total_Catch_lag4\"],\n",
        "    \"Melt_SST_Interaction_West_lag4\": df_model_with_interactions.iloc[-4][\"Melt_SST_Interaction_West_lag4\"],\n",
        "    \"Foreign_Catch_lag4\": df_model_with_interactions.iloc[-4][\"Foreign_Catch_lag4\"],\n",
        "    \"Melt_SST_Interaction_East_lag1\": df_model_with_interactions.iloc[-1][\"Melt_SST_Interaction_East_lag1\"],\n",
        "    \"Fish_Export_Value_Million_Kr_lag2\": df_model_with_interactions.iloc[-4][\"Fish_Export_Value_Million_Kr_lag2\"]\n",
        "}])\n",
        "\n",
        "# Step 3: LASSO pipeline\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "lasso_pipeline_q1 = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    Lasso(alpha=1.0, max_iter=10000)\n",
        ")\n",
        "\n",
        "lasso_pipeline_q1.fit(X_train_lasso_q1, y_train_lasso_q1)\n",
        "\n",
        "# Step 4: Predict\n",
        "y_pred_lasso_q1 = lasso_pipeline_q1.predict(X_nowcast_lasso_q1)[0]\n",
        "print(f\"📈 LASSO Nowcast for Q1 2025: {round(y_pred_lasso_q1):,.0f} tons\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph of Nowcast"
      ],
      "metadata": {
        "id": "OQy0L8MddqZ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgWKIHkIG8jP"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Rebuild time axis if needed\n",
        "df_clean[\"Time\"] = df_clean[\"Year\"].astype(str) + \" \" + df_clean[\"Quarter\"]\n",
        "df_clean[\"Time_Index\"] = range(len(df_clean))\n",
        "\n",
        "# Define the index and value for Q4 2024 (last actual) and Q1 2025 (nowcast)\n",
        "last_actual_index = df_clean[\"Time_Index\"].max()\n",
        "last_actual_value = df_clean[\"Total_Catch\"].iloc[-1]\n",
        "\n",
        "nowcast_index = last_actual_index + 1\n",
        "nowcast_value = y_pred_lasso_q1\n",
        "\n",
        "# Plot historical series\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df_clean[\"Time_Index\"], df_clean[\"Total_Catch\"], marker='o', linestyle='-', label=\"Historical\")\n",
        "\n",
        "# Add red dot for nowcast\n",
        "plt.plot(nowcast_index, nowcast_value, 'ro', label=\"LASSO Nowcast (Q1 2025)\", markersize=8)\n",
        "\n",
        "# Add red connecting line from Q4 2024 to Q1 2025\n",
        "plt.plot([last_actual_index, nowcast_index], [last_actual_value, nowcast_value], color='red', linestyle='-', linewidth=2)\n",
        "\n",
        "# Update x-ticks to include 2025\n",
        "xticks = list(df_clean[df_clean[\"Quarter\"] == \"Q1\"][\"Time_Index\"])\n",
        "xticks_labels = list(df_clean[df_clean[\"Quarter\"] == \"Q1\"][\"Year\"].astype(str))\n",
        "xticks.append(nowcast_index)\n",
        "xticks_labels.append(\"2025\")\n",
        "\n",
        "plt.xticks(ticks=xticks, labels=xticks_labels, rotation=45)\n",
        "\n",
        "plt.text(\n",
        "    nowcast_index + 0.2,          # shift right\n",
        "    nowcast_value + 2000,         # shift upward\n",
        "    f\"{round(nowcast_value):,}\",\n",
        "    color=\"red\",\n",
        "    fontsize=9.5,\n",
        ")\n",
        "\n",
        "plt.axvspan(nowcast_index - 0.1, nowcast_index + 0.1, color='red', alpha=0.1)\n",
        "\n",
        "plt.axvline(nowcast_index, color='gray', linestyle='--', linewidth=1)\n",
        "\n",
        "# Labels and formatting\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Total Catch (Tonnes)\")\n",
        "plt.title(\"Total Fish Catch Over Time with LASSO Nowcast\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('total_fish_catch_with_nowcast.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Capelin Dummy - Example of Policy Intervention"
      ],
      "metadata": {
        "id": "PpJm0uVwd-53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note on Capelin Dummy\n",
        "The Capelin_Open dummy variable is introduced here to model the impact of policy interventions (e.g., fishery closures in 2019, 2020, 2024, and 2025). This variable is used specifically for nowcasting in this notebook and is not included in the LASSO model fitting performed in model_fitting_diagnostics.ipynb. This allows us to isolate the effect of policy shocks in our nowcast analysis.\n"
      ],
      "metadata": {
        "id": "677-Byes6ICz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWNVEKSNIFDn"
      },
      "outputs": [],
      "source": [
        "# Add Capelin_Open dummy variable to the existing df_model_with_interactions\n",
        "df_model_capelin = df_model_with_interactions.copy()\n",
        "\n",
        "# Add Capelin_Open dummy variable\n",
        "df_model_capelin[\"Capelin_Open\"] = df_model_capelin[\"Year\"].apply(\n",
        "    lambda x: 0 if x in [2019, 2020, 2024, 2025] else 1\n",
        ")\n",
        "\n",
        "# Define target and features (include Capelin_Open)\n",
        "y_capelin = df_model_capelin[\"Total_Catch\"]\n",
        "X_capelin = df_model_capelin[\n",
        "    [\n",
        "        \"Total_Catch_lag4\",\n",
        "        \"Foreign_Catch_lag4\",\n",
        "        \"Melt_SST_Interaction_West_lag4\",\n",
        "        \"Melt_SST_Interaction_East_lag1\",\n",
        "        \"Fish_Export_Value_Million_Kr_lag2\",\n",
        "        \"Capelin_Open\"\n",
        "    ]\n",
        "]\n",
        "\n",
        "# Final check\n",
        "print(\"✅ X_capelin shape:\", X_capelin.shape)\n",
        "print(\"✅ y_capelin shape:\", y_capelin.shape)\n",
        "print(\"✅ Final predictors:\", X_capelin.columns.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nowcast with Capelin Dummy"
      ],
      "metadata": {
        "id": "f9p1NRDfeMqM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b48t71xQJrjh"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 1: Training data (up to Q4 2024)\n",
        "X_train_q1_2025_binary = df_model_capelin[\n",
        "    (df_model_capelin[\"Year\"] < 2025)\n",
        "][[\n",
        "    \"Total_Catch_lag4\",\n",
        "    \"Foreign_Catch_lag4\",\n",
        "    \"Melt_SST_Interaction_West_lag4\",\n",
        "    \"Melt_SST_Interaction_East_lag1\",\n",
        "    \"Fish_Export_Value_Million_Kr_lag2\",\n",
        "    \"Capelin_Open\"\n",
        "]]\n",
        "y_train_q1_2025_binary = df_model_capelin[\n",
        "    (df_model_capelin[\"Year\"] < 2025)\n",
        "][\"Total_Catch\"]\n",
        "\n",
        "print(\"✅ Training set size with Capelin dummy:\", X_train_q1_2025_binary.shape)\n",
        "\n",
        "# Step 2: Input row for Q1 2025\n",
        "X_nowcast_q1_2025_binary = pd.DataFrame([{\n",
        "    \"Total_Catch_lag4\": df_model_capelin.iloc[-4][\"Total_Catch_lag4\"],\n",
        "    \"Foreign_Catch_lag4\": df_model_capelin.iloc[-4][\"Foreign_Catch_lag4\"],\n",
        "    \"Melt_SST_Interaction_West_lag4\": df_model_capelin.iloc[-4][\"Melt_SST_Interaction_West_lag4\"],\n",
        "    \"Melt_SST_Interaction_East_lag1\": df_model_capelin.iloc[-1][\"Melt_SST_Interaction_East_lag1\"],\n",
        "    \"Fish_Export_Value_Million_Kr_lag2\": df_model_capelin.iloc[-4][\"Fish_Export_Value_Million_Kr_lag2\"],\n",
        "    \"Capelin_Open\": 0  # manually set to 0 because 2025 is closed\n",
        "}])\n",
        "\n",
        "print(\"✅ Nowcast input for Q1 2025 with Capelin dummy:\")\n",
        "display(X_nowcast_q1_2025_binary)\n",
        "\n",
        "# Step 3: Lasso model pipeline\n",
        "lasso_pipeline_q1_binary = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    Lasso(alpha=1.0, max_iter=50000, random_state=42)\n",
        ")\n",
        "\n",
        "# Step 4: Fit and predict\n",
        "lasso_pipeline_q1_binary.fit(X_train_q1_2025_binary, y_train_q1_2025_binary)\n",
        "y_pred_q1_2025_binary = lasso_pipeline_q1_binary.predict(X_nowcast_q1_2025_binary)[0]\n",
        "\n",
        "# Step 5: Output result\n",
        "print(f\"📈 📊 Lasso Nowcast for Q1 2025 (with Capelin dummy): {round(y_pred_q1_2025_binary):,.0f} tons\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graph Comparison of Nowcasts"
      ],
      "metadata": {
        "id": "mqG16rGLeUHN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrI2nK_nJr6d"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Rebuild time axis if needed\n",
        "df_clean[\"Time\"] = df_clean[\"Year\"].astype(str) + \" \" + df_clean[\"Quarter\"]\n",
        "df_clean[\"Time_Index\"] = range(len(df_clean))\n",
        "\n",
        "# Final observed point (Q4 2024)\n",
        "last_actual_index = df_clean[\"Time_Index\"].max()\n",
        "last_actual_value = df_clean[\"Total_Catch\"].iloc[-1]\n",
        "\n",
        "# Prediction point (Q1 2025)\n",
        "nowcast_index = last_actual_index + 1\n",
        "nowcast_std = y_pred_lasso_q1                 # Standard LASSO (no Capelin)\n",
        "nowcast_binary = y_pred_q1_2025_binary        # LASSO with Capelin dummy\n",
        "\n",
        "# Plot historical data\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df_clean[\"Time_Index\"], df_clean[\"Total_Catch\"], marker='o', linestyle='-', label=\"Historical\")\n",
        "\n",
        "# Plot both nowcasts as dots\n",
        "plt.plot(nowcast_index, nowcast_std, 'ro', label=\"LASSO Nowcast (No Capelin)\", markersize=8)\n",
        "plt.plot(nowcast_index, nowcast_binary, 'go', label=\"LASSO Nowcast (With Capelin)\", markersize=8)\n",
        "\n",
        "# Connect last actual to each forecast\n",
        "plt.plot([last_actual_index, nowcast_index], [last_actual_value, nowcast_std],\n",
        "         color='red', linestyle='-', linewidth=2)\n",
        "plt.plot([last_actual_index, nowcast_index], [last_actual_value, nowcast_binary],\n",
        "         color='green', linestyle='-', linewidth=2)\n",
        "\n",
        "# Annotate both nowcast values\n",
        "plt.text(nowcast_index + 0.2, nowcast_std + 2000,\n",
        "         f\"{round(nowcast_std):,}\", color=\"red\", fontsize=9.5)\n",
        "\n",
        "plt.text(nowcast_index + 0.2, nowcast_binary + 2000,\n",
        "         f\"{round(nowcast_binary):,}\", color=\"green\", fontsize=9.5)\n",
        "\n",
        "# Forecast area shading and vertical marker\n",
        "plt.axvspan(nowcast_index - 0.1, nowcast_index + 0.1, color='gray', alpha=0.1)\n",
        "plt.axvline(nowcast_index, color='gray', linestyle='--', linewidth=1)\n",
        "\n",
        "# X-axis setup\n",
        "xticks = list(df_clean[df_clean[\"Quarter\"] == \"Q1\"][\"Time_Index\"])\n",
        "xticks_labels = list(df_clean[df_clean[\"Quarter\"] == \"Q1\"][\"Year\"].astype(str))\n",
        "xticks.append(nowcast_index)\n",
        "xticks_labels.append(\"2025\")\n",
        "plt.xticks(ticks=xticks, labels=xticks_labels, rotation=45)\n",
        "\n",
        "# Labels and formatting\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Total Catch (Tonnes)\")\n",
        "plt.title(\"Total Fish Catch Over Time: LASSO Nowcast With and Without Capelin Dummy\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('nowcast_comparison.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actual vs. Predicted Plot for Backtests"
      ],
      "metadata": {
        "id": "DNerKdzLdebk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0FBO0cuElWz"
      },
      "outputs": [],
      "source": [
        "# Actual vs. Predicted Plot for Backtests\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Load the backtest results from Google Drive\n",
        "try:\n",
        "    backtest_results = pd.read_csv('/content/drive/MyDrive/backtest_results_2024.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Backtest results file not found. Ensure model_fitting_diagnostics.ipynb has been run to generate backtest_results_2024.csv.\")\n",
        "    raise\n",
        "\n",
        "# Step 2: Extract quarters, actual, and predicted values\n",
        "quarters = backtest_results[\"Quarter\"].tolist()\n",
        "actual_values = backtest_results[\"Actual\"].tolist()\n",
        "predicted_values = backtest_results[\"Predicted\"].tolist()\n",
        "\n",
        "# Step 3: Calculate y-axis padding\n",
        "all_values = actual_values + predicted_values\n",
        "y_min = min(all_values) * 0.9  # 10% below\n",
        "y_max = max(all_values) * 1.1  # 10% above\n",
        "\n",
        "# Step 4: Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(quarters, actual_values, label=\"Actual\", marker='o', linewidth=2)\n",
        "plt.plot(quarters, predicted_values, label=\"LASSO Prediction\", marker='o', linewidth=2)\n",
        "plt.ylim(y_min, y_max)\n",
        "plt.title(\"📊 Actual vs. Predicted Fish Catch (LASSO, Q1–Q4 2024)\")\n",
        "plt.xlabel(\"Quarter\")\n",
        "plt.ylabel(\"Total Catch (tons)\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('actual_vs_predicted.png')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Close the database connection\n",
        "try:\n",
        "    conn.close()\n",
        "    print(\"Database connection closed.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error closing database connection: {e}\")"
      ],
      "metadata": {
        "id": "lxBpcP6ESTqX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}